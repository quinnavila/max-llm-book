<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Feed-forward network - Build an LLM from scratch with MAX</title>


        <!-- Custom HTML head -->
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:ital,wght@0,100..700;1,100..700&display=swap"
            rel="stylesheet">
        
        <link rel="stylesheet" href="theme/css/custom.css">
        <link rel="stylesheet" href="theme/css/highlight.css">
        <link rel="stylesheet" id="theme">
        
        <!-- Additional meta tags -->
        <meta property="og:title" content="Build an LLM from scratch with MAX">
        <meta property="og:description" content="Learn by building LLMs from scratch with Modular's MAX Framework">
        <meta property="og:image" content="https://llm.modular.com/assets/social/modular-gpt-tutorial-metadata.png">
        <meta property="og:url" content="https://llm.modular.com/">
        <meta name="twitter:card" content="summary">
        <meta name="twitter:image:alt" content="MAX Docs">
        <meta name="twitter:title" content="Build an LLM from scratch with MAX">
        <meta name="twitter:description" content="Learn by building LLMs from scratch with Modular's MAX Framework">
        <meta name="twitter:image" content="https://llm.modular.com/assets/social/modular-gpt-tutorial-metadata.png">
        <link rel="icon" type="image/png" href="https://llm.modular.com/assets/icons/m-dark.svg">
        <script>
          !function(){var i="cioanalytics", analytics=(window[i]=window[i]||[]);if(!analytics.initialize)if(analytics.invoked)window.console&&console.error&&console.error("Snippet included twice.");else{analytics.invoked=!0;analytics.methods=["trackSubmit","trackClick","trackLink","trackForm","pageview","identify","reset","group","track","ready","alias","debug","page","once","off","on","addSourceMiddleware","addIntegrationMiddleware","setAnonymousId","addDestinationMiddleware"];analytics.factory=function(e){return function(){var t=Array.prototype.slice.call(arguments);t.unshift(e);analytics.push(t);return analytics}};for(var e=0;e<analytics.methods.length;e++){var key=analytics.methods[e];analytics[key]=analytics.factory(key)}analytics.load=function(key,e){var t=document.createElement("script");t.type="text/javascript";t.async=!0;t.setAttribute('data-global-customerio-analytics-key', i);t.src="https://cdp.customer.io/v1/analytics-js/snippet/" + key + "/analytics.min.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(t,n);analytics._writeKey=key;analytics._loadOptions=e};analytics.SNIPPET_VERSION="4.15.3";
            analytics.load(
              "c5c8ad95a28930735be9",
              {
                "integrations": {
                    "Customer.io In-App Plugin": {
                        anonymousInApp: true
                    }
                }
              }
            );
            analytics.page();
          }}();
        </script>

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&display=swap" rel="stylesheet">

        <meta name="description" content="This guide walks you through implementing GPT-2 using Modular’s MAX framework.">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/css/custom.css">
        <link rel="stylesheet" href="theme/css/highlight.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "ayu";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden" checked>
        <button id="vertical-sidebar-toggle" class="vertical-sidebar-toggle" aria-label="Toggle sidebar"></button>
        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <button class="collapse-sidebar" aria-label="Collapse sidebar"></button>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Toggle color mode and talk to us buttons -->
        <script>
            document.addEventListener('click', function (event) {
                if (!event.target.matches('.theme-toggle')) return;
                event.preventDefault();
                const prevTheme = theme;
                html.classList.remove(theme);
                const newTheme = prevTheme === 'ayu' ? 'light' : 'ayu'
                html.classList.add(newTheme);
                theme = newTheme
                localStorage.setItem('mdbook-theme', theme);
            }, false);
            document.addEventListener('click', function() {
                if (!event.target.matches('.log-in')) return;
                event.preventDefault();
                window.amplitude.logEvent('LoginClickedFromGPT2Tutorial');
                window.open('https://developer.modular.com', '_blank');
            });
        </script>

        <div class="page-header">
            <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                <i class="fa fa-bars"></i>
            </label>
            <div id="menu-bar" class="menu-bar">
                <div class="left-buttons">
                    <div class="logo-section">
                        <a class="desktop-logo-link" href="https://modular.com"></a>
                        <a class="mobile-logo-link" href="https://builds.modular.com"></a>
                        <div class="slash">/</div>
                        <a class="internal-link" href="">MAX LLM</a>
                    </div>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Dark</button></li>
                        </ul>
                    </div>
                <div class="right-buttons">
                    <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                        <i class="fa fa-search"></i>
                    </button>
                    <button id="theme-toggle" class="icon-button theme-toggle-btn" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="false" aria-expanded="false">
                        <i class="theme-toggle"></i>
                    </button>
                    <a class="menu-btn print" href="print.html" title="Print this book" aria-label="Print this book">
                        <i id="print-button" class="fa fa-print"></i>
                    </a>
                    <a class="menu-btn" href="https://github.com/modular/max-llm-book" title="Git repository" aria-label="Git repository">
                        <i id="git-repository-button" class="fa fa-github"></i>
                    </a>
                </div>
            </div>
        </div>

        <div id="page-wrapper" class="page-wrapper">
            <div class="page">

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <div id="content" class="content">
                    <main>
                        <h1 id="feed-forward-network-mlp"><a class="header" href="#feed-forward-network-mlp">Feed-forward network (MLP)</a></h1>
<div class="note">
<p>Learn to build the feed-forward network—also known as a multilayer perceptron
(MLP)—that processes information after attention in each transformer block.</p>
</div>
<p>In this step, you’ll create the <code>GPT2MLP</code> class: a two-layer feed-forward
network that appears after attention in every transformer block. The MLP expands
the embedding dimension by 4× (768 → 3,072), applies GELU activation for
non-linearity, then projects back to the original dimension.</p>
<p>While attention lets tokens communicate with each other, the MLP processes each
position independently. Attention aggregates information through weighted sums
(linear operations), but the MLP adds non-linearity through GELU activation.
This combination allows the model to learn complex patterns beyond what linear
transformations alone can capture.</p>
<p>GPT-2 uses a 4× expansion ratio (768 to 3,072 dimensions) because this was found
to work well in the original Transformer paper and has been validated across
many architectures since.</p>
<h2 id="understanding-the-components"><a class="header" href="#understanding-the-components">Understanding the components</a></h2>
<p>The MLP has three steps applied in sequence:</p>
<p><strong>Expansion layer (<code>c_fc</code>)</strong>: Projects from 768 to 3,072 dimensions using a
linear layer. This expansion gives the network more capacity to process
information.</p>
<p><strong>GELU activation</strong>: Applies Gaussian Error Linear Unit, a smooth non-linear
function. GPT-2 uses <code>approximate="tanh"</code> for the tanh-based approximation
instead of the exact computation. This approximation was faster when GPT-2 was
implemented, but while exact GELU is fast enough now, we use the approximation
to match the original weights.</p>
<p><strong>Projection layer (<code>c_proj</code>)</strong>: Projects back from 3,072 to 768 dimensions
using another linear layer. This returns to the embedding dimension so outputs
can be added to residual connections.</p>
<p>The layer names <code>c_fc</code> (fully connected) and <code>c_proj</code> (projection) match Hugging
Face’s GPT-2 checkpoint structure. This naming is essential for loading
pretrained weights.</p>
<div class="note">
<div class="title">MAX operations</div>
<p>You’ll use the following MAX operations to complete this task:</p>
<p><strong>Linear layers</strong>:</p>
<ul>
<li><a href="https://docs.modular.com/max/api/python/nn/module_v3#max.nn.Linear"><code>Linear(in_features, out_features, bias=True)</code></a>:
Applies linear transformation <code>y = xW^T + b</code></li>
</ul>
<p><strong>GELU activation</strong>:</p>
<ul>
<li><a href="https://docs.modular.com/max/api/python/nn/functional#max.nn.functional.gelu"><code>F.gelu(input, approximate="tanh")</code></a>:
Applies GELU activation with tanh approximation for faster computation</li>
</ul>
</div>
<h2 id="implementing-the-mlp"><a class="header" href="#implementing-the-mlp">Implementing the MLP</a></h2>
<p>You’ll create the <code>GPT2MLP</code> class that chains two linear layers with GELU
activation between them. The implementation is straightforward - three
operations applied in sequence.</p>
<p>First, import the required modules. You’ll need <code>functional as F</code> for the GELU
activation, <code>Tensor</code> for type hints, <code>Linear</code> for the layers, and <code>Module</code> as
the base class.</p>
<p>In the <code>__init__</code> method, create two linear layers:</p>
<ul>
<li>Expansion layer: <code>Linear(embed_dim, intermediate_size, bias=True)</code> stored as
<code>self.c_fc</code></li>
<li>Projection layer: <code>Linear(intermediate_size, embed_dim, bias=True)</code> stored as
<code>self.c_proj</code></li>
</ul>
<p>Both layers include bias terms (<code>bias=True</code>). The intermediate size is typically
4× the embedding dimension.</p>
<p>In the <code>forward</code> method, apply the three transformations:</p>
<ol>
<li>Expand: <code>hidden_states = self.c_fc(hidden_states)</code></li>
<li>Activate: <code>hidden_states = F.gelu(hidden_states, approximate="tanh")</code></li>
<li>Project: <code>hidden_states = self.c_proj(hidden_states)</code></li>
</ol>
<p>Return the final <code>hidden_states</code>. The input and output shapes are the same:
<code>[batch, seq_length, embed_dim]</code>.</p>
<p><strong>Implementation</strong> (<code>step_02.py</code>):</p>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Step 02: Feed-forward Network (MLP)

Implement the MLP used in each transformer block with GELU activation.

Tasks:
1. Import functional (as F), Tensor, Linear, and Module from MAX
2. Create c_fc linear layer (embedding to intermediate dimension)
3. Create c_proj linear layer (intermediate back to embedding dimension)
4. Apply c_fc transformation in forward pass
5. Apply GELU activation function
6. Apply c_proj transformation and return result

Run: pixi run s02
"""

# 1: Import the required modules from MAX
# TODO: Import functional module from max.nn with the alias F
# https://docs.modular.com/max/api/python/nn/functional

# TODO: Import Tensor from max.tensor
# https://docs.modular.com/max/api/python/tensor.Tensor

# TODO: Import Linear and Module from max.nn
# https://docs.modular.com/max/api/python/nn/module_v3

from max.tensor import Tensor
from step_01 import GPT2Config


class GPT2MLP(Module):
    """Feed-forward network matching HuggingFace GPT-2 structure.

    Args:
        intermediate_size: Size of the intermediate layer.
        config: GPT-2 configuration.
    """

    def __init__(self, intermediate_size: int, config: GPT2Config) -&gt; None:
        super().__init__()
        embed_dim = config.n_embd

        # 2: Create the first linear layer (embedding to intermediate)
        # TODO: Create self.c_fc as a Linear layer from embed_dim to intermediate_size with bias=True
        # https://docs.modular.com/max/api/python/nn/module_v3#max.nn.Linear
        # Hint: This is the expansion layer in the MLP
        self.c_fc = None

        # 3: Create the second linear layer (intermediate back to embedding)
        # TODO: Create self.c_proj as a Linear layer from intermediate_size to embed_dim with bias=True
        # https://docs.modular.com/max/api/python/nn/module_v3#max.nn.Linear
        # Hint: This is the projection layer that brings us back to the embedding dimension
        self.c_proj = None

    def forward(self, hidden_states: Tensor) -&gt; Tensor:
        """Apply feed-forward network.

        Args:
            hidden_states: Input hidden states.

        Returns:
            MLP output.
        """
        # 4: Apply the first linear transformation
        # TODO: Apply self.c_fc to hidden_states
        # Hint: This expands the hidden dimension to the intermediate size
        hidden_states = None

        # 5: Apply GELU activation function
        # TODO: Use F.gelu() with hidden_states and approximate="tanh"
        # https://docs.modular.com/max/api/python/nn/functional#max.nn.functional.gelu
        # Hint: GELU is the non-linear activation used in GPT-2's MLP
        hidden_states = None

        # 6: Apply the second linear transformation and return
        # TODO: Apply self.c_proj to hidden_states and return the result
        # Hint: This projects back to the embedding dimension
        return None
</code></pre>
<h3 id="validation"><a class="header" href="#validation">Validation</a></h3>
<p>Run <code>pixi run s02</code> to verify your implementation.</p>
<details>
<summary>Show solution</summary>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Solution for Step 02: Feed-forward Network (MLP)

This module implements the feed-forward network (MLP) used in each
transformer block with GELU activation.
"""

import max.functional as F
from max.nn import Linear, Module
from max.tensor import Tensor
from step_01 import GPT2Config


class GPT2MLP(Module):
    """Feed-forward network matching HuggingFace GPT-2 structure.

    Args:
        intermediate_size: Size of the intermediate layer.
        config: GPT-2 configuration.
    """

    def __init__(self, intermediate_size: int, config: GPT2Config) -&gt; None:
        super().__init__()
        embed_dim = config.n_embd
        self.c_fc = Linear(embed_dim, intermediate_size, bias=True)
        self.c_proj = Linear(intermediate_size, embed_dim, bias=True)

    def forward(self, hidden_states: Tensor) -&gt; Tensor:
        """Apply feed-forward network.

        Args:
            hidden_states: Input hidden states.

        Returns:
            MLP output.
        """
        hidden_states = self.c_fc(hidden_states)
        hidden_states = F.gelu(hidden_states, approximate="tanh")
        hidden_states = self.c_proj(hidden_states)
        return hidden_states
</code></pre>
</details>
<p><strong>Next</strong>: In <a href="./step_03.html">Step 03</a>, you’ll implement causal masking to prevent
tokens from attending to future positions in autoregressive generation.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="step_01.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="step_03.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="step_01.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="step_03.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="theme/code-highlighting.js"></script>
        <script src="theme/sidebar.js"></script>
        <script src="theme/init-amplitude.js"></script>
        <script src="theme/warning.js"></script>


    </div>
    </body>
</html>