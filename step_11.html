<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Load weights and run model - Build an LLM from scratch with MAX</title>


        <!-- Custom HTML head -->
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:ital,wght@0,100..700;1,100..700&display=swap"
            rel="stylesheet">
        
        <link rel="stylesheet" href="theme/css/custom.css">
        <link rel="stylesheet" href="theme/css/highlight.css">
        <link rel="stylesheet" id="theme">
        
        <!-- Additional meta tags -->
        <meta property="og:title" content="Build an LLM from scratch with MAX">
        <meta property="og:description" content="Learn by building LLMs from scratch with Modular's MAX Framework">
        <meta property="og:image" content="https://llm.modular.com/assets/social/modular-gpt-tutorial-metadata.png">
        <meta property="og:url" content="https://llm.modular.com/">
        <meta name="twitter:card" content="summary">
        <meta name="twitter:image:alt" content="MAX Docs">
        <meta name="twitter:title" content="Build an LLM from scratch with MAX">
        <meta name="twitter:description" content="Learn by building LLMs from scratch with Modular's MAX Framework">
        <meta name="twitter:image" content="https://llm.modular.com/assets/social/modular-gpt-tutorial-metadata.png">
        <link rel="icon" type="image/png" href="https://llm.modular.com/assets/icons/m-dark.svg">
        <script>
          !function(){var i="cioanalytics", analytics=(window[i]=window[i]||[]);if(!analytics.initialize)if(analytics.invoked)window.console&&console.error&&console.error("Snippet included twice.");else{analytics.invoked=!0;analytics.methods=["trackSubmit","trackClick","trackLink","trackForm","pageview","identify","reset","group","track","ready","alias","debug","page","once","off","on","addSourceMiddleware","addIntegrationMiddleware","setAnonymousId","addDestinationMiddleware"];analytics.factory=function(e){return function(){var t=Array.prototype.slice.call(arguments);t.unshift(e);analytics.push(t);return analytics}};for(var e=0;e<analytics.methods.length;e++){var key=analytics.methods[e];analytics[key]=analytics.factory(key)}analytics.load=function(key,e){var t=document.createElement("script");t.type="text/javascript";t.async=!0;t.setAttribute('data-global-customerio-analytics-key', i);t.src="https://cdp.customer.io/v1/analytics-js/snippet/" + key + "/analytics.min.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(t,n);analytics._writeKey=key;analytics._loadOptions=e};analytics.SNIPPET_VERSION="4.15.3";
            analytics.load(
              "c5c8ad95a28930735be9",
              {
                "integrations": {
                    "Customer.io In-App Plugin": {
                        anonymousInApp: true
                    }
                }
              }
            );
            analytics.page();
          }}();
        </script>

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&display=swap" rel="stylesheet">

        <meta name="description" content="This guide walks you through implementing GPT-2 using Modular’s MAX framework.">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/css/custom.css">
        <link rel="stylesheet" href="theme/css/highlight.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "ayu";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden" checked>
        <button id="vertical-sidebar-toggle" class="vertical-sidebar-toggle" aria-label="Toggle sidebar"></button>
        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <button class="collapse-sidebar" aria-label="Collapse sidebar"></button>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Toggle color mode and talk to us buttons -->
        <script>
            document.addEventListener('click', function (event) {
                if (!event.target.matches('.theme-toggle')) return;
                event.preventDefault();
                const prevTheme = theme;
                html.classList.remove(theme);
                const newTheme = prevTheme === 'ayu' ? 'light' : 'ayu'
                html.classList.add(newTheme);
                theme = newTheme
                localStorage.setItem('mdbook-theme', theme);
            }, false);
            document.addEventListener('click', function() {
                if (!event.target.matches('.log-in')) return;
                event.preventDefault();
                window.amplitude.logEvent('LoginClickedFromGPT2Tutorial');
                window.open('https://developer.modular.com', '_blank');
            });
        </script>

        <div class="page-header">
            <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                <i class="fa fa-bars"></i>
            </label>
            <div id="menu-bar" class="menu-bar">
                <div class="left-buttons">
                    <div class="logo-section">
                        <a class="desktop-logo-link" href="https://modular.com"></a>
                        <a class="mobile-logo-link" href="https://builds.modular.com"></a>
                        <div class="slash">/</div>
                        <a class="internal-link" href="">MAX LLM</a>
                    </div>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Dark</button></li>
                        </ul>
                    </div>
                <div class="right-buttons">
                    <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                        <i class="fa fa-search"></i>
                    </button>
                    <button id="theme-toggle" class="icon-button theme-toggle-btn" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="false" aria-expanded="false">
                        <i class="theme-toggle"></i>
                    </button>
                    <a class="menu-btn print" href="print.html" title="Print this book" aria-label="Print this book">
                        <i id="print-button" class="fa fa-print"></i>
                    </a>
                    <a class="menu-btn" href="https://github.com/modular/max-llm-book" title="Git repository" aria-label="Git repository">
                        <i id="git-repository-button" class="fa fa-github"></i>
                    </a>
                </div>
            </div>
        </div>

        <div id="page-wrapper" class="page-wrapper">
            <div class="page">

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <div id="content" class="content">
                    <main>
                        <h1 id="load-weights-and-run-model"><a class="header" href="#load-weights-and-run-model">Load weights and run model</a></h1>
<div class="note">
<p>Learn to load pretrained weights from HuggingFace and prepare the model for text
generation.</p>
</div>
<p>With all components implemented, you’re ready to load OpenAI’s pretrained GPT-2
weights and run the model. This step brings everything together: loading weights
from HuggingFace, handling weight format differences, initializing the
tokenizer, and compiling the model for efficient inference.</p>
<p>The HuggingFace <code>transformers</code> library provides OpenAI’s pretrained GPT-2
weights. You’ll load these weights into your MAX implementation, making your
model immediately capable of generating coherent text without training.</p>
<p>However, there’s a complication: HuggingFace’s GPT-2 uses Conv1D layers for its
linear transformations, while your MAX implementation uses standard Linear
layers. These store weights in transposed formats, so you’ll need to transpose
specific weight matrices after loading.</p>
<h2 id="understanding-weight-loading"><a class="header" href="#understanding-weight-loading">Understanding weight loading</a></h2>
<p>Weight loading involves three steps: loading the HuggingFace model, transferring
weights to your MAX model, and transposing Conv1D weights.</p>
<p>First, load the pretrained model with <code>GPT2LMHeadModel.from_pretrained("gpt2")</code>.
This downloads the weights (about 500MB) and returns a PyTorch model with the
exact architecture you’ve implemented.</p>
<p>Next, transfer these weights to your MAX model using
<code>max_model.load_state_dict(hf_model.state_dict())</code>. The <code>state_dict</code> is a
dictionary mapping layer names to weight tensors. Since your MAX model has the
exact same architecture and layer names, this transfer works seamlessly.</p>
<p>Finally, transpose the weights for layers that use Conv1D in HuggingFace:
<code>c_attn</code>, <code>c_proj</code>, and <code>c_fc</code>. Conv1D stores weights in shape
<code>[in_features, out_features]</code>, while Linear expects
<code>[out_features, in_features]</code>. Use the <code>.T</code> property to transpose:
<code>child.weight = child.weight.T</code>.</p>
<h2 id="understanding-model-compilation"><a class="header" href="#understanding-model-compilation">Understanding model compilation</a></h2>
<p>Before you can run text generation, compile the model with
<code>.compile(token_type)</code>. Compilation analyzes the model’s computation graph and
generates optimized code for your hardware.</p>
<p>First, you need to specify the <code>token_type</code> input using <code>TensorType</code>. This tells
the MAX compiler what shape and dtype to expect:</p>
<pre><code class="language-python">token_type = TensorType(
    DType.int64,
    ("batch", "seqlen"),
    device=DeviceRef.from_device(device)
)
</code></pre>
<p>The shape uses symbolic dimensions <code>("batch", "seqlen")</code> rather than concrete
numbers like <code>[1, 20]</code>. This allows the compiled model to handle any batch size
and sequence length, not just fixed dimensions.</p>
<p>Compilation takes a few seconds but only happens once. After compilation,
inference is much faster because MAX has optimized the entire computation graph.</p>
<h2 id="understanding-the-tokenizer"><a class="header" href="#understanding-the-tokenizer">Understanding the tokenizer</a></h2>
<p>Back in step 9, you implemented functions to encode and decode tokens, but both
functions require a <code>tokenizer</code> argument. Now you’ll load that tokenizer from
Hugging Face, using <code>GPT2Tokenizer.from_pretrained("gpt2")</code>,
which downloads the same tokenization rules OpenAI used during training.</p>
<p>Set the padding token to match the end-of-sequence token:
<code>tokenizer.pad_token = tokenizer.eos_token</code>. GPT-2 doesn’t have a dedicated
padding token, so we reuse the EOS token for this purpose.</p>
<p>Then pass the <code>tokenizer</code> to the <code>generate_text()</code> function you created
in step 10 (which passes it to <code>tokenize_text()</code> and <code>decode_tokens()</code>
from step 9).</p>
<h2 id="implementing-the-main-function"><a class="header" href="#implementing-the-main-function">Implementing the main function</a></h2>
<p>You’ll implement the <code>main()</code> function that orchestrates the entire pipeline:
loading models, transferring weights, initializing the tokenizer, compiling the
model, and running an interactive prompt loop.</p>
<p>Start by loading the pretrained HuggingFace model:</p>
<pre><code class="language-python">hf_model = GPT2LMHeadModel.from_pretrained("gpt2")
</code></pre>
<p>Initialize your MAX model with the default device and configuration:</p>
<pre><code class="language-python">_, device = defaults()
config = GPT2Config()
max_model = MaxGPT2LMHeadModel(config)
</code></pre>
<p>The <code>defaults()</code> function returns <code>(dtype, device)</code> tuples. You only need the
device, so use <code>_</code> to ignore the dtype.</p>
<p>Load and transpose the weights:</p>
<pre><code class="language-python">max_model.load_state_dict(hf_model.state_dict())
max_model.to(device)
for name, child in max_model.descendents:
    if isinstance(child, Linear):
        if any(layer_name in name for layer_name in ["c_attn", "c_proj", "c_fc"]):
            child.weight = child.weight.T
</code></pre>
<p>The <code>descendents</code> property gives you all nested modules with their full paths.
Check each child’s name for the Conv1D layers and transpose their weights.</p>
<p>Initialize the tokenizer:</p>
<pre><code class="language-python">tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token
</code></pre>
<p>Compile the model:</p>
<pre><code class="language-python">token_type = TensorType(
    DType.int64, ("batch", "seqlen"), device=DeviceRef.from_device(device)
)
compiled_max_model = max_model.compile(token_type)
</code></pre>
<p>Finally, create an interactive prompt loop where users can input text and see
generated results:</p>
<pre><code class="language-python">try:
    while True:
        user_input = input("Enter your prompt: ").strip()

        if user_input.lower() in ['quit', 'exit', 'q']:
            break

        if not user_input:
            continue

        generated_text = generate_text(
            compiled_max_model,
            tokenizer,
            device,
            user_input,
            max_new_tokens=50,
            temperature=0.8,
            do_sample=True
        )
        print(f"\nGenerated text:\n{generated_text}\n")

except KeyboardInterrupt:
    print("\n\nExiting...")
</code></pre>
<p>The loop continues until the user types ‘quit’, ‘exit’, ‘q’, or presses Ctrl+C.</p>
<p><strong>Implementation</strong> (<code>step_11.py</code>):</p>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Step 11: Load Weights and Run Model

Load pretrained GPT-2 weights from HuggingFace and run the complete model.

Tasks:
1. Load HuggingFace GPT-2 model and weights
2. Initialize MAX model and load state dict
3. Transpose weights for Conv1D-&gt;Linear compatibility
4. Compile model with correct input specification
5. Create interactive generation loop

Run: pixi run s11
"""


def run_model() -&gt; None:
    """Load GPT-2 model, compile it, and run interactive text generation."""

    # TODO: Load HuggingFace model
    # Hint: hf_model = GPT2LMHeadModel.from_pretrained("gpt2")
    # Hint: print(f"Loaded HuggingFace model:\n{hf_model}")
    hf_model = None

    # TODO: Initialize MAX model with device
    # Hint: _, device = defaults()
    # Hint: print(f"Using device: {device}")
    # Hint: config = GPT2Config()
    # Hint: max_model = MaxGPT2LMHeadModel(config)
    device = None
    config = None
    max_model = None

    print(
        f"Model has {config.n_layer} layers, {config.n_head} heads, {config.n_embd} embedding dim"
    )

    # TODO: Load state dict and move to device
    # Hint: max_model.load_state_dict(hf_model.state_dict())
    # Hint: max_model.to(device)

    # TODO: Transpose weights for Linear layers
    # Hint: HuggingFace uses Conv1D which stores weights transposed
    # Hint: for name, child in max_model.descendents:
    #     if isinstance(child, Linear):
    #         if any(layer_name in name for layer_name in ["c_attn", "c_proj", "c_fc"]):
    #             print(f"Transposing {name}: {child.weight.shape}")
    #             child.weight = child.weight.T

    # TODO: Initialize tokenizer
    # Hint: tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    # Hint: tokenizer.pad_token = tokenizer.eos_token
    tokenizer = None

    # TODO: Compile model
    # Hint: print("\nCompiling model...")
    # Hint: Create TensorType with shape ("batch", "seqlen") and int64 dtype
    # Hint: token_type = TensorType(DType.int64, ("batch", "seqlen"), device=DeviceRef.from_device(device))
    # Hint: compiled_max_model = max_model.compile(token_type)
    compiled_max_model = None

    # Interactive prompt loop
    print("\n" + "=" * 50)
    print("Model ready! Enter prompts to generate text.")
    print("Press Ctrl+C or type 'quit' to exit.")
    print("=" * 50 + "\n")

    # TODO: Implement interactive generation loop
    # Hint: try:
    #     while True:
    #         user_input = input("Enter your prompt: ").strip()
    #         if user_input.lower() in ['quit', 'exit', 'q']:
    #             break
    #         if not user_input:
    #             continue
    #         generated_text = generate_text(
    #             compiled_max_model, tokenizer, device, user_input,
    #             max_new_tokens=50, temperature=0.8, do_sample=True
    #         )
    #         print(f"\nGenerated text:\n{generated_text}\n")
    # except KeyboardInterrupt:
    #     print("\n\nExiting...")


if __name__ == "__main__":
    run_model()
</code></pre>
<h3 id="validation"><a class="header" href="#validation">Validation</a></h3>
<p>Run <code>pixi run s11</code> to verify your implementation.</p>
<details>
<summary>Show solution</summary>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Solution for Step 11: Load weights and run model


"""

from max.dtype import DType
from max.graph import DeviceRef
from max.nn import Linear
from max.tensor import TensorType, defaults
from step_01 import GPT2Config
from step_08 import MaxGPT2LMHeadModel
from step_10 import generate_text
from transformers import GPT2LMHeadModel, GPT2Tokenizer


def run_model() -&gt; None:
    # Load HuggingFace model
    hf_model = GPT2LMHeadModel.from_pretrained("gpt2")
    print(f"Loaded HuggingFace model:\n{hf_model}")

    # Initialize Max model
    _, device = defaults()
    print(f"Using device: {device}")
    config = GPT2Config()
    max_model = MaxGPT2LMHeadModel(config)

    print(
        f"Model has {config.n_layer} layers, {config.n_head} heads, {config.n_embd} embedding dim"
    )

    # Load state dict and transpose weights
    max_model.load_state_dict(hf_model.state_dict())
    max_model.to(device)
    for name, child in max_model.descendants:
        if isinstance(child, Linear):
            if any(layer_name in name for layer_name in ["c_attn", "c_proj", "c_fc"]):
                print(f"Transposing {name}: {child.weight.shape}")
                # The upstream model has conv1d layers instead of linear, which have their weights
                # stored transposed compared to linear
                child.weight = child.weight.T

    # Initialize tokenizer
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    tokenizer.pad_token = tokenizer.eos_token  # Set padding token

    # Compile model
    print("\nCompiling model...")
    token_type = TensorType(
        DType.int64, ("batch", "seqlen"), device=DeviceRef.from_device(device)
    )
    compiled_max_model = max_model.compile(token_type)

    # Interactive prompt loop
    print("\n" + "=" * 50)
    print("Model ready! Enter prompts to generate text.")
    print("Press Ctrl+C or type 'quit' to exit.")
    print("=" * 50 + "\n")

    try:
        while True:
            user_input = input("Enter your prompt: ").strip()

            if user_input.lower() in ["quit", "exit", "q"]:
                print("Exiting...")
                break

            if not user_input:
                print("Please enter a non-empty prompt.\n")
                continue

            print()
            generated_text = generate_text(
                compiled_max_model,
                tokenizer,
                device,
                user_input,
                max_new_tokens=50,
                temperature=0.8,
                do_sample=True,
            )
            print(f"\nGenerated text:\n{generated_text}\n")
            print("-" * 50 + "\n")

    except KeyboardInterrupt:
        print("\n\nExiting...")


if __name__ == "__main__":
    run_model()
</code></pre>
</details>
<p><strong>Congratulations!</strong> You’ve completed built a complete GPT-2 implementation from
scratch.</p>
<p>If code verification passed, you can execute your <code>step_11.py</code> code with
<code>pixi run gpt2</code>.</p>
<h2 id="whats-next"><a class="header" href="#whats-next">What’s next?</a></h2>
<p>You now understand the architectural foundation that powers modern language
models. LLaMA, Mistral, and more build on these same components with incremental
refinements. You have everything you need to implement those refinements
yourself.</p>
<p>Consider extending your implementation with:</p>
<ul>
<li><strong>Grouped-query attention (GQA)</strong>: Reduce memory consumption by sharing
key-value pairs across multiple query heads, as used in LLaMA 2.</li>
<li><strong>Rotary position embeddings (RoPE)</strong>: Replace learned position embeddings
with rotation-based encoding, improving length extrapolation in models like
LLaMA and GPT-NeoX.</li>
<li><strong>SwiGLU activation</strong>: Swap GELU for the gated linear unit variant used in
LLaMA and PaLM.</li>
<li><strong>Mixture of experts (MoE)</strong>: Add sparse expert routing to scale model
capacity efficiently, as in Mixtral and GPT-4.</li>
</ul>
<p>Each refinement builds directly on what you’ve implemented. The attention
mechanism you wrote becomes grouped-query attention with a simple modification
to how you reshape key-value tensors. Your position embeddings can be replaced
with RoPE by changing how you encode positional information. The feed-forward
network you built becomes SwiGLU by adding a gating mechanism.</p>
<p>Pick an architecture that interests you and start building. You’ll find the
patterns are familiar because the fundamentals haven’t changed.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="step_10.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="step_10.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="theme/code-highlighting.js"></script>
        <script src="theme/sidebar.js"></script>
        <script src="theme/init-amplitude.js"></script>
        <script src="theme/warning.js"></script>


    </div>
    </body>
</html>