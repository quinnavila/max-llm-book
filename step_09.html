<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Encode and decode tokens - Build an LLM from scratch with MAX</title>


        <!-- Custom HTML head -->
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:ital,wght@0,100..700;1,100..700&display=swap"
            rel="stylesheet">
        
        <link rel="stylesheet" href="theme/css/custom.css">
        <link rel="stylesheet" href="theme/css/highlight.css">
        <link rel="stylesheet" id="theme">
        
        <!-- Additional meta tags -->
        <meta property="og:title" content="Build an LLM from scratch with MAX">
        <meta property="og:description" content="Learn by building LLMs from scratch with Modular's MAX Framework">
        <meta property="og:image" content="https://llm.modular.com/assets/social/modular-gpt-tutorial-metadata.png">
        <meta property="og:url" content="https://llm.modular.com/">
        <meta name="twitter:card" content="summary">
        <meta name="twitter:image:alt" content="MAX Docs">
        <meta name="twitter:title" content="Build an LLM from scratch with MAX">
        <meta name="twitter:description" content="Learn by building LLMs from scratch with Modular's MAX Framework">
        <meta name="twitter:image" content="https://llm.modular.com/assets/social/modular-gpt-tutorial-metadata.png">
        <link rel="icon" type="image/png" href="https://llm.modular.com/assets/icons/m-dark.svg">
        <script>
          !function(){var i="cioanalytics", analytics=(window[i]=window[i]||[]);if(!analytics.initialize)if(analytics.invoked)window.console&&console.error&&console.error("Snippet included twice.");else{analytics.invoked=!0;analytics.methods=["trackSubmit","trackClick","trackLink","trackForm","pageview","identify","reset","group","track","ready","alias","debug","page","once","off","on","addSourceMiddleware","addIntegrationMiddleware","setAnonymousId","addDestinationMiddleware"];analytics.factory=function(e){return function(){var t=Array.prototype.slice.call(arguments);t.unshift(e);analytics.push(t);return analytics}};for(var e=0;e<analytics.methods.length;e++){var key=analytics.methods[e];analytics[key]=analytics.factory(key)}analytics.load=function(key,e){var t=document.createElement("script");t.type="text/javascript";t.async=!0;t.setAttribute('data-global-customerio-analytics-key', i);t.src="https://cdp.customer.io/v1/analytics-js/snippet/" + key + "/analytics.min.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(t,n);analytics._writeKey=key;analytics._loadOptions=e};analytics.SNIPPET_VERSION="4.15.3";
            analytics.load(
              "c5c8ad95a28930735be9",
              {
                "integrations": {
                    "Customer.io In-App Plugin": {
                        anonymousInApp: true
                    }
                }
              }
            );
            analytics.page();
          }}();
        </script>

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&display=swap" rel="stylesheet">

        <meta name="description" content="This guide walks you through implementing GPT-2 using Modular’s MAX framework.">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/css/custom.css">
        <link rel="stylesheet" href="theme/css/highlight.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "ayu";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden" checked>
        <button id="vertical-sidebar-toggle" class="vertical-sidebar-toggle" aria-label="Toggle sidebar"></button>
        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <button class="collapse-sidebar" aria-label="Collapse sidebar"></button>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Toggle color mode and talk to us buttons -->
        <script>
            document.addEventListener('click', function (event) {
                if (!event.target.matches('.theme-toggle')) return;
                event.preventDefault();
                const prevTheme = theme;
                html.classList.remove(theme);
                const newTheme = prevTheme === 'ayu' ? 'light' : 'ayu'
                html.classList.add(newTheme);
                theme = newTheme
                localStorage.setItem('mdbook-theme', theme);
            }, false);
            document.addEventListener('click', function() {
                if (!event.target.matches('.log-in')) return;
                event.preventDefault();
                window.amplitude.logEvent('LoginClickedFromGPT2Tutorial');
                window.open('https://developer.modular.com', '_blank');
            });
        </script>

        <div class="page-header">
            <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                <i class="fa fa-bars"></i>
            </label>
            <div id="menu-bar" class="menu-bar">
                <div class="left-buttons">
                    <div class="logo-section">
                        <a class="desktop-logo-link" href="https://modular.com"></a>
                        <a class="mobile-logo-link" href="https://builds.modular.com"></a>
                        <div class="slash">/</div>
                        <a class="internal-link" href="">MAX LLM</a>
                    </div>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Dark</button></li>
                        </ul>
                    </div>
                <div class="right-buttons">
                    <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                        <i class="fa fa-search"></i>
                    </button>
                    <button id="theme-toggle" class="icon-button theme-toggle-btn" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="false" aria-expanded="false">
                        <i class="theme-toggle"></i>
                    </button>
                    <a class="menu-btn print" href="print.html" title="Print this book" aria-label="Print this book">
                        <i id="print-button" class="fa fa-print"></i>
                    </a>
                    <a class="menu-btn" href="https://github.com/modular/max-llm-book" title="Git repository" aria-label="Git repository">
                        <i id="git-repository-button" class="fa fa-github"></i>
                    </a>
                </div>
            </div>
        </div>

        <div id="page-wrapper" class="page-wrapper">
            <div class="page">

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <div id="content" class="content">
                    <main>
                        <h1 id="encode-and-decode-tokens"><a class="header" href="#encode-and-decode-tokens">Encode and decode tokens</a></h1>
<div class="note">
<p>Learn to convert between text and token IDs using tokenizers and MAX tensors.</p>
</div>
<p>In this step, you’ll implement utility functions to bridge the gap between text
and the token IDs your model operates on. The <code>encode_text()</code> function converts
an input string into a tensor of token IDs, while <code>decode_tokens()</code> converts
token IDs into a string.</p>
<p>As you saw when building the model body in step 7 (<code>MaxGPT2Model</code>), the model
must receive input as token IDs (not raw text). The token IDs are integers that
represent pieces of text according to a tokenizer vocabulary. GPT-2 uses a Byte
Pair Encoding (BPE) tokenizer, which breaks text into subword units. For
example, “Hello world” becomes <code>[15496, 995]</code> - two tokens representing the
words.</p>
<p>You’ll use the Hugging Face tokenizer to handle the text-to-token conversion,
then wrap it with functions that work with MAX tensors. This separation keeps
tokenization (a preprocessing step) separate from model inference (tensor
operations).</p>
<h2 id="understanding-tokenization"><a class="header" href="#understanding-tokenization">Understanding tokenization</a></h2>
<p>Tokenization converts text to a list of integers. The GPT-2 tokenizer uses a
vocabulary of 50,257 tokens, where common words get single tokens and rare words
split into subwords.</p>
<p>The HuggingFace tokenizer provides an <code>encode</code> method that takes text and
returns a Python list of token IDs. For example:</p>
<pre><code class="language-python">token_ids = tokenizer.encode("Hello world")  # Returns [15496, 995]
</code></pre>
<p>You can specify <code>max_length</code> and <code>truncation=True</code> to limit sequence length. If
the text exceeds <code>max_length</code>, the tokenizer cuts it off. This prevents memory
issues with very long inputs.</p>
<p>After encoding, you need to convert the Python list to a MAX tensor. Use
<code>Tensor.constant</code> to create a tensor with the token IDs, specifying
<code>dtype=DType.int64</code> (GPT-2 expects 64-bit integers) and the target device.</p>
<p>The tensor needs shape <code>[batch, seq_length]</code> for model input. Wrap the token
list in another list to add the batch dimension: <code>[token_ids]</code> becomes
<code>[[15496, 995]]</code> with shape <code>[1, 2]</code>.</p>
<h2 id="understanding-decoding"><a class="header" href="#understanding-decoding">Understanding decoding</a></h2>
<p>Decoding reverses tokenization: convert token IDs back to text. This requires
moving tensors from GPU to CPU, converting to NumPy, then using the tokenizer’s
<code>decode</code> method.</p>
<p>First, transfer the tensor to CPU with <code>.to(CPU())</code>. MAX tensors can live on GPU
or CPU, but Python libraries like NumPy only work with CPU data.</p>
<p>Next, convert to NumPy using <code>np.from_dlpack</code>. DLPack is a standard that enables
zero-copy tensor sharing between frameworks. The MAX tensor and NumPy array
share the same underlying memory.</p>
<p>If the tensor is 2D (batch dimension present), flatten it to 1D with
<code>.flatten()</code>. The tokenizer expects a flat list of token IDs, not a batched
format.</p>
<p>Finally, convert to a Python list with <code>.tolist()</code> and decode with
<code>tokenizer.decode(token_ids, skip_special_tokens=True)</code>. The
<code>skip_special_tokens=True</code> parameter removes padding and end-of-sequence markers
from the output.</p>
<div class="note">
<div class="title">MAX operations</div>
<p>You’ll use the following MAX operations to complete this task:</p>
<p><strong>Tensor creation</strong>:</p>
<ul>
<li><a href="https://docs.modular.com/max/api/python/tensor#max.tensor.Tensor.constant"><code>Tensor.constant(data, dtype, device)</code></a>:
Creates tensor from Python data</li>
</ul>
<p><strong>Device transfer</strong>:</p>
<ul>
<li><a href="https://docs.modular.com/max/api/python/tensor#max.tensor.Tensor.to"><code>tensor.to(CPU())</code></a>:
Moves tensor to CPU for NumPy conversion</li>
</ul>
<p><strong>NumPy interop</strong>:</p>
<ul>
<li><code>np.from_dlpack(tensor)</code>: Converts MAX tensor to NumPy using DLPack protocol</li>
</ul>
</div>
<h2 id="implementing-tokenization"><a class="header" href="#implementing-tokenization">Implementing tokenization</a></h2>
<p>You’ll create two functions: <code>encode_text</code> to convert strings to tensors, and
<code>decode_tokens</code> to convert tensors back to strings.</p>
<p>First, import the required modules. You’ll need <code>numpy as np</code> for array
operations, <code>CPU</code> from MAX’s driver for device specification, <code>DType</code> for
specifying integer types, and <code>Tensor</code> for creating and manipulating tensors.</p>
<p>In <code>encode_text</code>, implement the encoding and conversion:</p>
<ol>
<li>Encode the text to token IDs using the tokenizer:
<code>token_ids = tokenizer.encode(text, max_length=max_length, truncation=True)</code></li>
<li>Convert to a MAX tensor with batch dimension:
<code>Tensor.constant([token_ids], dtype=DType.int64, device=device)</code></li>
</ol>
<p>Note the <code>[token_ids]</code> wrapping to create the batch dimension. This gives shape
<code>[1, seq_length]</code> instead of just <code>[seq_length]</code>.</p>
<p>In <code>decode_tokens</code>, implement the reverse process with explicit type conversions:</p>
<ol>
<li>Transfer to CPU and convert to NumPy with explicit type annotation:
<code>token_ids_np: np.ndarray = np.from_dlpack(token_ids.to(CPU()))</code></li>
<li>Flatten if needed: <code>if token_ids_np.ndim &gt; 1: token_ids_np = token_ids_np.flatten()</code></li>
<li>Convert to Python list with explicit type annotation:
<code>token_ids_list: list = token_ids_np.tolist()</code></li>
<li>Decode to text: <code>return tokenizer.decode(token_ids_list, skip_special_tokens=True)</code></li>
</ol>
<p>Note the use of separate variable names (<code>token_ids_np</code>, <code>token_ids_list</code>)
instead of reusing the same variable. This makes the type conversions explicit
and improves code clarity: <code>Tensor</code> → <code>np.ndarray</code> → <code>list</code> → <code>str</code>. The
flattening step handles both 1D and 2D tensors, making the function work with
single sequences or batches.</p>
<p><strong>Implementation</strong> (<code>step_09.py</code>):</p>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Step 09: Encode and decode tokens

This module provides utility functions to tokenize input text
and decode token IDs back to text using a tokenizer.

Tasks:
1. Tokenize text and convert to tensor
2. Decode token IDs back to text

Run: pixi run s09
"""

# TODO: Import required modules
# Hint: You'll need numpy as np
# Hint: You'll need CPU from max.driver
# Hint: You'll need DType from max.dtype
# Hint: You'll need Tensor from max.tensor

from max.driver import Device
from max.tensor import Tensor
from transformers import GPT2Tokenizer


def encode_text(
    text: str, tokenizer: GPT2Tokenizer, device: Device, max_length: int = 128
) -&gt; Tensor:
    """Tokenize text and convert to tensor.

    Args:
        text: Input text to tokenize
        tokenizer: HuggingFace tokenizer
        device: Device to place tensor on
        max_length: Maximum sequence length

    Returns:
        Tensor of token IDs with shape [1, seq_length]
    """
    # TODO: Encode text to token IDs
    # Hint: token_ids = tokenizer.encode(text, max_length=max_length, truncation=True)
    pass

    # TODO: Convert to MAX tensor
    # Hint: return Tensor.constant([token_ids], dtype=DType.int64, device=device)
    # Note: Wrap tokens in a list to create batch dimension
    return None


def decode_tokens(token_ids: Tensor, tokenizer: GPT2Tokenizer) -&gt; str:
    """Decode token IDs back to text.

    Args:
        token_ids: Tensor of token IDs
        tokenizer: HuggingFace tokenizer

    Returns:
        Decoded text string
    """
    # TODO: Convert MAX tensor to NumPy array explicitly
    # Hint: Create a new variable with type annotation: token_ids_np: np.ndarray
    # Hint: token_ids_np = np.from_dlpack(token_ids.to(CPU()))
    # Note: This makes the type conversion from Tensor to np.ndarray explicit
    pass

    # TODO: Flatten if needed
    # Hint: if token_ids_np.ndim &gt; 1: token_ids_np = token_ids_np.flatten()
    pass

    # TODO: Convert to Python list explicitly
    # Hint: Create a new variable: token_ids_list: list = token_ids_np.tolist()
    # Note: This makes the conversion from np.ndarray to list explicit
    pass

    # TODO: Decode to text
    # Hint: return tokenizer.decode(token_ids_list, skip_special_tokens=True)
    return None
</code></pre>
<h3 id="validation"><a class="header" href="#validation">Validation</a></h3>
<p>Run <code>pixi run s09</code> to verify your implementation correctly converts text to
tensors and back.</p>
<details>
<summary>Show solution</summary>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Solution for Step 09: Encode and decode tokens

This module provides utility functions to tokenize input text
and decode token IDs back to text using a tokenizer.
"""

import numpy as np
from max.driver import CPU, Device
from max.dtype import DType
from max.tensor import Tensor
from transformers import GPT2Tokenizer


def encode_text(
    text: str, tokenizer: GPT2Tokenizer, device: Device, max_length: int = 128
) -&gt; Tensor:
    """Tokenize text and convert to tensor."""
    token_ids = tokenizer.encode(text, max_length=max_length, truncation=True)
    return Tensor.constant([token_ids], dtype=DType.int64, device=device)


def decode_tokens(token_ids: Tensor, tokenizer: GPT2Tokenizer) -&gt; str:
    """Decode token IDs back to text."""
    token_ids_np: np.ndarray = np.from_dlpack(token_ids.to(CPU()))
    if token_ids_np.ndim &gt; 1:
        token_ids_np = token_ids_np.flatten()
    token_ids_list: list = token_ids_np.tolist()
    return tokenizer.decode(token_ids_list, skip_special_tokens=True)
</code></pre>
</details>
<p><strong>Next</strong>: In <a href="./step_10.html">Step 10</a>, you’ll implement the text generation loop
that uses these functions to produce coherent text autoregressively.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="step_08.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="step_10.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="step_08.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="step_10.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="theme/code-highlighting.js"></script>
        <script src="theme/sidebar.js"></script>
        <script src="theme/init-amplitude.js"></script>
        <script src="theme/warning.js"></script>


    </div>
    </body>
</html>