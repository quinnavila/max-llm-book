<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Build an LLM from scratch with MAX</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:ital,wght@0,100..700;1,100..700&display=swap"
            rel="stylesheet">
        
        <link rel="stylesheet" href="theme/css/custom.css">
        <link rel="stylesheet" href="theme/css/highlight.css">
        <link rel="stylesheet" id="theme">
        
        <!-- Additional meta tags -->
        <meta property="og:title" content="Build an LLM from scratch with MAX">
        <meta property="og:description" content="Learn by building LLMs from scratch with Modular's MAX Framework">
        <meta property="og:image" content="https://llm.modular.com/assets/social/modular-gpt-tutorial-metadata.png">
        <meta property="og:url" content="https://llm.modular.com/">
        <meta name="twitter:card" content="summary">
        <meta name="twitter:image:alt" content="MAX Docs">
        <meta name="twitter:title" content="Build an LLM from scratch with MAX">
        <meta name="twitter:description" content="Learn by building LLMs from scratch with Modular's MAX Framework">
        <meta name="twitter:image" content="https://llm.modular.com/assets/social/modular-gpt-tutorial-metadata.png">
        <link rel="icon" type="image/png" href="https://llm.modular.com/assets/icons/m-dark.svg">
        <script>
          !function(){var i="cioanalytics", analytics=(window[i]=window[i]||[]);if(!analytics.initialize)if(analytics.invoked)window.console&&console.error&&console.error("Snippet included twice.");else{analytics.invoked=!0;analytics.methods=["trackSubmit","trackClick","trackLink","trackForm","pageview","identify","reset","group","track","ready","alias","debug","page","once","off","on","addSourceMiddleware","addIntegrationMiddleware","setAnonymousId","addDestinationMiddleware"];analytics.factory=function(e){return function(){var t=Array.prototype.slice.call(arguments);t.unshift(e);analytics.push(t);return analytics}};for(var e=0;e<analytics.methods.length;e++){var key=analytics.methods[e];analytics[key]=analytics.factory(key)}analytics.load=function(key,e){var t=document.createElement("script");t.type="text/javascript";t.async=!0;t.setAttribute('data-global-customerio-analytics-key', i);t.src="https://cdp.customer.io/v1/analytics-js/snippet/" + key + "/analytics.min.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(t,n);analytics._writeKey=key;analytics._loadOptions=e};analytics.SNIPPET_VERSION="4.15.3";
            analytics.load(
              "c5c8ad95a28930735be9",
              {
                "integrations": {
                    "Customer.io In-App Plugin": {
                        anonymousInApp: true
                    }
                }
              }
            );
            analytics.page();
          }}();
        </script>

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&display=swap" rel="stylesheet">

        <meta name="description" content="This guide walks you through implementing GPT-2 using Modular’s MAX framework.">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/css/custom.css">
        <link rel="stylesheet" href="theme/css/highlight.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "ayu";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden" checked>
        <button id="vertical-sidebar-toggle" class="vertical-sidebar-toggle" aria-label="Toggle sidebar"></button>
        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <button class="collapse-sidebar" aria-label="Collapse sidebar"></button>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Toggle color mode and talk to us buttons -->
        <script>
            document.addEventListener('click', function (event) {
                if (!event.target.matches('.theme-toggle')) return;
                event.preventDefault();
                const prevTheme = theme;
                html.classList.remove(theme);
                const newTheme = prevTheme === 'ayu' ? 'light' : 'ayu'
                html.classList.add(newTheme);
                theme = newTheme
                localStorage.setItem('mdbook-theme', theme);
            }, false);
            document.addEventListener('click', function() {
                if (!event.target.matches('.log-in')) return;
                event.preventDefault();
                window.amplitude.logEvent('LoginClickedFromGPT2Tutorial');
                window.open('https://developer.modular.com', '_blank');
            });
        </script>

        <div class="page-header">
            <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                <i class="fa fa-bars"></i>
            </label>
            <div id="menu-bar" class="menu-bar">
                <div class="left-buttons">
                    <div class="logo-section">
                        <a class="desktop-logo-link" href="https://modular.com"></a>
                        <a class="mobile-logo-link" href="https://builds.modular.com"></a>
                        <div class="slash">/</div>
                        <a class="internal-link" href="">MAX LLM</a>
                    </div>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Dark</button></li>
                        </ul>
                    </div>
                <div class="right-buttons">
                    <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                        <i class="fa fa-search"></i>
                    </button>
                    <button id="theme-toggle" class="icon-button theme-toggle-btn" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="false" aria-expanded="false">
                        <i class="theme-toggle"></i>
                    </button>
                    <a class="menu-btn print" href="print.html" title="Print this book" aria-label="Print this book">
                        <i id="print-button" class="fa fa-print"></i>
                    </a>
                    <a class="menu-btn" href="https://github.com/modular/max-llm-book" title="Git repository" aria-label="Git repository">
                        <i id="git-repository-button" class="fa fa-github"></i>
                    </a>
                </div>
            </div>
        </div>

        <div id="page-wrapper" class="page-wrapper">
            <div class="page">

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <div id="content" class="content">
                    <main>
                        <h1 id="build-an-llm-from-scratch-in-max"><a class="header" href="#build-an-llm-from-scratch-in-max">Build an LLM from scratch in MAX</a></h1>
<p>Transformer models power today’s most impactful AI applications, from language
models like ChatGPT to code generation tools like GitHub Copilot. Maybe you’ve
been asked to adapt one of these models for your team, or you want to understand
what’s actually happening when you call an inference API. Either way, building
a transformer from scratch is one of the best ways to truly understand how they
work.</p>
<p>This guide walks you through implementing GPT-2 using the
<a href="https://docs.modular.com/max/api/python/">MAX Python API</a>. You’ll
build each component yourself: embeddings, attention mechanisms, and feed-forward
layers. You’ll see how they fit together into a complete language model by
completing the sequential coding challenges in the tutorial
<a href="https://github.com/modular/max-llm-book">GitHub repository</a>.</p>
<h2 id="why-gpt-2"><a class="header" href="#why-gpt-2">Why GPT-2?</a></h2>
<p>It’s the architectural foundation for modern language models. LLaMA, Mistral,
GPT-4; they’re all built on the same core components you’ll implement here:</p>
<ul>
<li>multi-head attention</li>
<li>feed-forward layers</li>
<li>layer normalization</li>
</ul>
<p>Modern variants add refinements like grouped-query attention or mixture of
experts, but the fundamentals remain the same. GPT-2 is complex enough to teach
real transformer architecture but simple enough to implement completely and
understand deeply. When you grasp how its pieces fit together, you understand
how to build any transformer-based model.</p>
<blockquote>
<p><strong>Learning by building</strong>: This tutorial follows a format popularized by Andrej
Karpathy’s educational work and Sebastian Raschka’s hands-on approach. Rather
than abstract theory, you’ll implement each component yourself, building
intuition through practice.</p>
</blockquote>
<h2 id="why-max"><a class="header" href="#why-max">Why MAX?</a></h2>
<p>Traditional ML development often feels like stitching together tools that
weren’t designed to work together. Maybe you write your model in PyTorch,
optimize in CUDA, convert to ONNX for deployment, then use separate serving
tools. Each handoff introduces complexity.</p>
<p>MAX Framework takes a different approach: everything happens in one unified
system. You write code to define your model, load weights, and run inference,
all in MAX’s Python API. The MAX Framework handles optimization automatically and
you can even use MAX Serve to manage your deployment.</p>
<p>When you build GPT-2 in this guide, you’ll load pretrained weights from
Hugging Face, implement the architecture, and run text generation, all in the same
environment.</p>
<h2 id="why-coding-challenges"><a class="header" href="#why-coding-challenges">Why coding challenges?</a></h2>
<p>This tutorial emphasizes <strong>active problem-solving over passive reading</strong>. Each
step presents a focused implementation task with:</p>
<ol>
<li><strong>Clear context</strong>: What you’re building and why it matters</li>
<li><strong>Guided implementation</strong>: Code structure with specific tasks to complete</li>
<li><strong>Immediate validation</strong>: Tests that verify correctness before moving forward</li>
<li><strong>Conceptual grounding</strong>: Explanations that connect code to architecture</li>
</ol>
<p>Rather than presenting complete solutions, this approach helps you develop
intuition for <strong>when</strong> and <strong>why</strong> to use specific patterns. The skills you
build extend beyond GPT-2 to model development more broadly.</p>
<p>You can work through the tutorial sequentially for comprehensive understanding,
or skip directly to topics you need. Each step is self-contained enough to be
useful independently while building toward a complete implementation.</p>
<h2 id="what-youll-build"><a class="header" href="#what-youll-build">What you’ll build</a></h2>
<p>This tutorial guides you through building GPT-2 in manageable steps:</p>
<div class="table-wrapper"><table><thead><tr><th>Step</th><th>Component</th><th>What you’ll learn</th></tr></thead><tbody>
<tr><td>1</td><td><a href="./step_01.html">Model configuration</a></td><td>Define architecture hyperparameters matching HuggingFace GPT-2.</td></tr>
<tr><td>2</td><td><a href="./step_02.html">Feed-forward network</a></td><td>Build the position-wise feed-forward network with GELU activation.</td></tr>
<tr><td>3</td><td><a href="./step_03.html">Causal masking</a></td><td>Create attention masks to prevent looking at future tokens.</td></tr>
<tr><td>4</td><td><a href="./step_04.html">Multi-head attention</a></td><td>Implement scaled dot-product attention with multiple heads.</td></tr>
<tr><td>5</td><td><a href="./step_05.html">Layer normalization</a></td><td>Ensure activation values are within a stable range.</td></tr>
<tr><td>6</td><td><a href="./step_06.html">Transformer block</a></td><td>Combine attention and MLP with residual connections.</td></tr>
<tr><td>7</td><td><a href="./step_07.html">Stacking transformer blocks</a></td><td>Create the complete 12-layer GPT-2 model with embeddings.</td></tr>
<tr><td>8</td><td><a href="./step_08.html">Language model head</a></td><td>Project hidden states to vocabulary logits.</td></tr>
<tr><td>9</td><td><a href="./step_09.html">Encode and decode tokens</a></td><td>Convert between text and token IDs using HuggingFace tokenizer.</td></tr>
<tr><td>10</td><td><a href="./step_10.html">Text generation</a></td><td>Generate text autoregressively with temperature sampling.</td></tr>
<tr><td>11</td><td><a href="./step_11.html">Load weights and run model</a></td><td>Load pretrained weights and interact with your complete model.</td></tr>
</tbody></table>
</div>
<p>By the end, you’ll have a complete GPT-2 implementation and practical experience
with MAX’s Python API. These are skills you can immediately apply to your own projects.</p>
<blockquote>
<p><strong>Note on training vs. inference</strong>: While some steps reference concepts from
training (like layer normalization for “stabilizing activations”), this
tutorial focuses on inference using pretrained weights from Hugging Face.
Training is not in scope, but we include these architectural details for
learning purposes and completeness—understanding why each layer exists helps
you reason about model behavior and adapt architectures for your own needs.</p>
</blockquote>
<h2 id="try-it-first"><a class="header" href="#try-it-first">Try it first</a></h2>
<p>Before diving into the implementation, you can experience what you’ll build by
running the complete reference model:</p>
<pre><code class="language-bash">pixi run main
</code></pre>
<p>This runs the complete GPT-2 implementation from
<a href="https://github.com/modular/max-llm-book/blob/main/main.py"><code>main.py</code></a>, loading
pretrained weights and starting an interactive prompt where you can enter text
and see the model generate completions. It’s the same model you’ll build
step-by-step through the tutorial.</p>
<p>When you’ve completed every step of the tutorial, you can run your own
implementation the exact same way:</p>
<pre><code class="language-bash">pixi run gpt2
</code></pre>
<p>This runs your completed <code>steps/step_11.py</code>, demonstrating that your
implementation works identically to the reference. Both commands load the same
pretrained weights, compile the model, and provide an interactive generation
experience.</p>
<h2 id="get-started"><a class="header" href="#get-started">Get started</a></h2>
<p>To install the tutorial and begin building, follow the steps in
<a href="./setup.html">Setup</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="project-setup"><a class="header" href="#project-setup">Project Setup</a></h1>
<p>You’ll first need to clone
<a href="https://github.com/modular/max-llm-book">the GitHub repository</a> and navigate to
the repository:</p>
<pre><code class="language-sh">git clone https://github.com/modular/max-llm-book
cd max-llm-book
</code></pre>
<p>Then download and install <a href="https://pixi.sh/dev/">pixi</a>:</p>
<pre><code class="language-sh">curl -fsSL https://pixi.sh/install.sh | sh
</code></pre>
<h2 id="how-to-use-the-book"><a class="header" href="#how-to-use-the-book">How to use the book</a></h2>
<p>To validate a step, use the corresponding check command. For example, to check
Step 01:</p>
<pre><code class="language-bash">pixi run s01
</code></pre>
<p>Each step includes automated checks that verify your implementation before moving
forward. This immediate feedback helps you catch issues early and build
confidence. Initially, checks will fail because the implementation isn’t complete:</p>
<pre><code class="language-text">✨ Pixi task (s01): python checks/check_step_01.py
Running checks for Step 01: Model Configuration...

✅ GPT2Config can be instantiated with default values

❌ ERRORS:
  - GPT2Config must be a dataclass (use @dataclass decorator)
  - Field 'vocab_size' has incorrect value: expected 50257, got None
  - Field 'n_positions' has incorrect value: expected 1024, got None
# ...
</code></pre>
<p>Each failure tells you exactly what to implement.</p>
<p>When your implementation is
correct, you’ll see:</p>
<pre><code class="language-text">✨ Pixi task (s01): python checks/check_step_01.py
Running checks for Step 01: Model Configuration...

✅ GPT2Config is a dataclass
✅ GPT2Config can be instantiated with default values
✅ vocab_size = 50257
✅ n_positions = 1024
# ...
</code></pre>
<p>The check output tells you exactly what needs to be fixed, making it easy to
iterate until your implementation is correct. Once all checks pass, you’re ready
to move on to the next step.</p>
<h2 id="a-note-on-compile-times"><a class="header" href="#a-note-on-compile-times">A note on compile times</a></h2>
<p>Compile times are actively being improved. As MAX continues to evolve, you
should expect performance improvements alongside upcoming Modular releases.</p>
<h2 id="using-code-assistants"><a class="header" href="#using-code-assistants">Using code assistants</a></h2>
<p>Code assistants like <a href="https://claude.ai">Claude</a>, <a href="https://cursor.sh">Cursor</a>,
or similar tools can help you navigate this tutorial. They’re particularly
useful for:</p>
<ul>
<li><strong>Explaining concepts</strong>: Ask about transformer architecture, attention
mechanisms, or any step in the tutorial</li>
<li><strong>Understanding the MAX API</strong>: Get clarification on MAX Framework methods,
parameters, and patterns</li>
<li><strong>Debugging check failures</strong>: Paste check output to understand what’s missing</li>
<li><strong>Exploring alternatives</strong>: Ask “why this approach?” to deepen your understanding</li>
</ul>
<p>If you’re using Claude, see <a href="./claude.html">claude.md</a> for custom instructions
tailored to this tutorial.</p>
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<p>This tutorial assumes:</p>
<ul>
<li><strong>Basic Python knowledge</strong>: Classes, functions, type hints</li>
<li><strong>Familiarity with neural networks</strong>: What embeddings and layers do (we’ll
explain the specifics)</li>
<li><strong>Interest in understanding</strong>: Curiosity matters more than prior transformer
experience</li>
</ul>
<p>Whether you’re exploring MAX for the first time or deepening your understanding
of model architecture, this tutorial provides hands-on experience you can apply
to current projects and learning priorities.</p>
<p>Ready to build? Let’s get started with
<a href="./step_01.html">Step 01: Model configuration</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="model-configuration"><a class="header" href="#model-configuration">Model configuration</a></h1>
<div class="note">
<p>Learn to define the GPT-2 model architecture parameters using configuration classes.</p>
</div>
<p>Before you can implement GPT-2, you need to define its architecture: the
dimensions, layer counts, and structural parameters that determine how the model
processes information.</p>
<p>In this step, you’ll create <code>GPT2Config</code>, a class that holds all the
architectural decisions for GPT-2. This class describes things like: embedding
dimensions, number of transformer layers, and number of attention heads. These
parameters define the shape and capacity of your model.</p>
<p>OpenAI trained the original GPT-2 model with specific parameters that you can
see in the
<a href="https://huggingface.co/openai-community/gpt2/blob/main/config.json">config.json file</a>
on Hugging Face. By using the exact same values, we can access OpenAI’s
pretrained weights in subsequent steps.</p>
<h2 id="understanding-the-parameters"><a class="header" href="#understanding-the-parameters">Understanding the parameters</a></h2>
<p>Looking at the
<a href="https://huggingface.co/openai-community/gpt2/blob/main/config.json">config.json file</a>
file, we can see some key information about the model. Each parameter controls
a different aspect of the model’s architecture:</p>
<ul>
<li><code>vocab_size</code>: Size of the token vocabulary (default: 50,257). This seemingly
odd number is actually 50,000 Byte Pair Encoding (BPE) tokens + 256 byte-level
tokens (fallback for rare characters) + 1 special token.</li>
<li><code>n_positions</code>: Maximum sequence length, also called the context window
(default: 1,024). Longer sequences require quadratic memory in attention.</li>
<li><code>n_embd</code>: Embedding dimension, or the size of the hidden states that flow
through the model (default: 768). This determines the model’s capacity to
represent information.</li>
<li><code>n_layer</code>: Number of transformer blocks stacked vertically (default: 12). More
layers allow the model to learn more complex patterns.</li>
<li><code>n_head</code>: Number of attention heads per layer (default: 12). Multiple heads
let the model attend to different types of patterns simultaneously.</li>
<li><code>n_inner</code>: Dimension of the MLP intermediate layer (default: 3,072). This is
4x the embedding dimension, a ratio found empirically in the
<a href="https://arxiv.org/abs/1706.03762"><em>Attention is all you need</em></a> paper to work
well.</li>
<li><code>layer_norm_epsilon</code>: Small constant for numerical stability in layer
normalization (default: <code>1e-5</code>). This prevents division by zero when variance
is very small.</li>
</ul>
<p>These values define the <em>small</em> GPT-2 model. OpenAI released four sizes (small,
medium, large, XL), each with different configurations that scale up these
parameters. For implementation purposes we will use these parameters.</p>
<h2 id="implementing-the-configuration"><a class="header" href="#implementing-the-configuration">Implementing the configuration</a></h2>
<p>Now let’s implement this yourself. You’ll create the <code>GPT2Config</code> class using
Python’s <a href="https://docs.python.org/3/library/dataclasses.html"><code>@dataclass</code></a>
decorator. Dataclasses reduce boilerplate.</p>
<p>Instead of writing <code>__init__</code> and defining each parameter manually, you just
declare the fields with type hints and default values.</p>
<p>First, you’ll need to import the dataclass decorator from the dataclasses
module. Then you’ll add the <code>@dataclass</code> decorator to the <code>GPT2Config</code> class
definition.</p>
<p>The actual parameter values come from Hugging Face. You can get them in two ways:</p>
<ul>
<li><strong>Option 1</strong>: Run <code>pixi run huggingface</code> to access these parameters
programmatically from the Hugging Face <code>transformers</code> library.</li>
<li><strong>Option 2</strong>: Read the values directly from the
<a href="https://huggingface.co/openai-community/gpt2/blob/main/config.json">GPT-2 model card</a>.</li>
</ul>
<p>Once you have the values, replace each <code>None</code> in the <code>GPT2Config</code> class
properties with the correct numbers from the configuration.</p>
<p><strong>Implementation</strong> (<code>step_01.py</code>):</p>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Step 01: Model Configuration

Implement the GPT-2 configuration dataclass that stores model hyperparameters.

Tasks:
1. Import dataclass from the dataclasses module
2. Add the @dataclass decorator to the GPT2Config class
3. Fill in the configuration values from HuggingFace GPT-2 model

Run: pixi run s01
"""

# 1. Import dataclass from the dataclasses module

# 2. Add the Python @dataclass decorator to the GPT2Config class


class GPT2Config:
    """GPT-2 configuration matching HuggingFace.

    Attributes:
        vocab_size: Size of the vocabulary.
        n_positions: Maximum sequence length.
        n_embd: Embedding dimension.
        n_layer: Number of transformer layers.
        n_head: Number of attention heads.
        n_inner: Inner dimension of feed-forward network (defaults to 4 * n_embd if None).
        layer_norm_epsilon: Epsilon for layer normalization.
    """

    # 3a. Run `pixi run huggingface` to access the model parameters from the Hugging Face `transformers` library
    # 3b. Alternately, read the values from GPT-2 model card: https://huggingface.co/openai-community/gpt2/blob/main/config.json
    # 4. Replace the None of the GPT2Config properties with the correct values
    vocab_size: int = None
    n_positions: int = None
    n_embd: int = None
    n_layer: int = None
    n_head: int = None
    n_inner: int = None  # Equal to 4 * n_embd
    layer_norm_epsilon: float = None
</code></pre>
<h3 id="validation"><a class="header" href="#validation">Validation</a></h3>
<p>Run <code>pixi run s01</code> to verify your implementation matches the expected configuration.</p>
<details>
<summary>Show solution</summary>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""Solution for Step 01: Model Configuration

This module implements the GPT-2 configuration dataclass that stores
hyperparameters matching HuggingFace's GPT-2 model structure.
"""

from dataclasses import dataclass


@dataclass
class GPT2Config:
    """GPT-2 configuration matching HuggingFace.

    Attributes:
        vocab_size: Size of the vocabulary.
        n_positions: Maximum sequence length.
        n_embd: Embedding dimension.
        n_layer: Number of transformer layers.
        n_head: Number of attention heads.
        n_inner: Inner dimension of feed-forward network (defaults to 4 * n_embd if None).
        layer_norm_epsilon: Epsilon for layer normalization.
    """

    vocab_size: int = 50257
    n_positions: int = 1024
    n_embd: int = 768
    n_layer: int = 12
    n_head: int = 12
    n_inner: int = 3072
    layer_norm_epsilon: float = 1e-5
</code></pre>
</details>
<p><strong>Next</strong>: In <a href="./step_02.html">Step 02</a>, you’ll implement the feed-forward
network—also known as a multilayer perceptron (MLP)—that processes information
after attention in each transformer block.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="feed-forward-network-mlp"><a class="header" href="#feed-forward-network-mlp">Feed-forward network (MLP)</a></h1>
<div class="note">
<p>Learn to build the feed-forward network—also known as a multilayer perceptron
(MLP)—that processes information after attention in each transformer block.</p>
</div>
<p>In this step, you’ll create the <code>GPT2MLP</code> class: a two-layer feed-forward
network that appears after attention in every transformer block. The MLP expands
the embedding dimension by 4× (768 → 3,072), applies GELU activation for
non-linearity, then projects back to the original dimension.</p>
<p>While attention lets tokens communicate with each other, the MLP processes each
position independently. Attention aggregates information through weighted sums
(linear operations), but the MLP adds non-linearity through GELU activation.
This combination allows the model to learn complex patterns beyond what linear
transformations alone can capture.</p>
<p>GPT-2 uses a 4× expansion ratio (768 to 3,072 dimensions) because this was found
to work well in the original Transformer paper and has been validated across
many architectures since.</p>
<h2 id="understanding-the-components"><a class="header" href="#understanding-the-components">Understanding the components</a></h2>
<p>The MLP has three steps applied in sequence:</p>
<p><strong>Expansion layer (<code>c_fc</code>)</strong>: Projects from 768 to 3,072 dimensions using a
linear layer. This expansion gives the network more capacity to process
information.</p>
<p><strong>GELU activation</strong>: Applies Gaussian Error Linear Unit, a smooth non-linear
function. GPT-2 uses <code>approximate="tanh"</code> for the tanh-based approximation
instead of the exact computation. This approximation was faster when GPT-2 was
implemented, but while exact GELU is fast enough now, we use the approximation
to match the original weights.</p>
<p><strong>Projection layer (<code>c_proj</code>)</strong>: Projects back from 3,072 to 768 dimensions
using another linear layer. This returns to the embedding dimension so outputs
can be added to residual connections.</p>
<p>The layer names <code>c_fc</code> (fully connected) and <code>c_proj</code> (projection) match Hugging
Face’s GPT-2 checkpoint structure. This naming is essential for loading
pretrained weights.</p>
<div class="note">
<div class="title">MAX operations</div>
<p>You’ll use the following MAX operations to complete this task:</p>
<p><strong>Linear layers</strong>:</p>
<ul>
<li><a href="https://docs.modular.com/max/api/python/nn/module_v3#max.nn.Linear"><code>Linear(in_features, out_features, bias=True)</code></a>:
Applies linear transformation <code>y = xW^T + b</code></li>
</ul>
<p><strong>GELU activation</strong>:</p>
<ul>
<li><a href="https://docs.modular.com/max/api/python/nn/functional#max.nn.functional.gelu"><code>F.gelu(input, approximate="tanh")</code></a>:
Applies GELU activation with tanh approximation for faster computation</li>
</ul>
</div>
<h2 id="implementing-the-mlp"><a class="header" href="#implementing-the-mlp">Implementing the MLP</a></h2>
<p>You’ll create the <code>GPT2MLP</code> class that chains two linear layers with GELU
activation between them. The implementation is straightforward - three
operations applied in sequence.</p>
<p>First, import the required modules. You’ll need <code>functional as F</code> for the GELU
activation, <code>Tensor</code> for type hints, <code>Linear</code> for the layers, and <code>Module</code> as
the base class.</p>
<p>In the <code>__init__</code> method, create two linear layers:</p>
<ul>
<li>Expansion layer: <code>Linear(embed_dim, intermediate_size, bias=True)</code> stored as
<code>self.c_fc</code></li>
<li>Projection layer: <code>Linear(intermediate_size, embed_dim, bias=True)</code> stored as
<code>self.c_proj</code></li>
</ul>
<p>Both layers include bias terms (<code>bias=True</code>). The intermediate size is typically
4× the embedding dimension.</p>
<p>In the <code>forward</code> method, apply the three transformations:</p>
<ol>
<li>Expand: <code>hidden_states = self.c_fc(hidden_states)</code></li>
<li>Activate: <code>hidden_states = F.gelu(hidden_states, approximate="tanh")</code></li>
<li>Project: <code>hidden_states = self.c_proj(hidden_states)</code></li>
</ol>
<p>Return the final <code>hidden_states</code>. The input and output shapes are the same:
<code>[batch, seq_length, embed_dim]</code>.</p>
<p><strong>Implementation</strong> (<code>step_02.py</code>):</p>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Step 02: Feed-forward Network (MLP)

Implement the MLP used in each transformer block with GELU activation.

Tasks:
1. Import functional (as F), Tensor, Linear, and Module from MAX
2. Create c_fc linear layer (embedding to intermediate dimension)
3. Create c_proj linear layer (intermediate back to embedding dimension)
4. Apply c_fc transformation in forward pass
5. Apply GELU activation function
6. Apply c_proj transformation and return result

Run: pixi run s02
"""

# 1: Import the required modules from MAX
# TODO: Import functional module from max.nn with the alias F
# https://docs.modular.com/max/api/python/nn/functional

# TODO: Import Tensor from max.tensor
# https://docs.modular.com/max/api/python/tensor.Tensor

# TODO: Import Linear and Module from max.nn
# https://docs.modular.com/max/api/python/nn/module_v3

from max.tensor import Tensor
from step_01 import GPT2Config


class GPT2MLP(Module):
    """Feed-forward network matching HuggingFace GPT-2 structure.

    Args:
        intermediate_size: Size of the intermediate layer.
        config: GPT-2 configuration.
    """

    def __init__(self, intermediate_size: int, config: GPT2Config) -&gt; None:
        super().__init__()
        embed_dim = config.n_embd

        # 2: Create the first linear layer (embedding to intermediate)
        # TODO: Create self.c_fc as a Linear layer from embed_dim to intermediate_size with bias=True
        # https://docs.modular.com/max/api/python/nn/module_v3#max.nn.Linear
        # Hint: This is the expansion layer in the MLP
        self.c_fc = None

        # 3: Create the second linear layer (intermediate back to embedding)
        # TODO: Create self.c_proj as a Linear layer from intermediate_size to embed_dim with bias=True
        # https://docs.modular.com/max/api/python/nn/module_v3#max.nn.Linear
        # Hint: This is the projection layer that brings us back to the embedding dimension
        self.c_proj = None

    def forward(self, hidden_states: Tensor) -&gt; Tensor:
        """Apply feed-forward network.

        Args:
            hidden_states: Input hidden states.

        Returns:
            MLP output.
        """
        # 4: Apply the first linear transformation
        # TODO: Apply self.c_fc to hidden_states
        # Hint: This expands the hidden dimension to the intermediate size
        hidden_states = None

        # 5: Apply GELU activation function
        # TODO: Use F.gelu() with hidden_states and approximate="tanh"
        # https://docs.modular.com/max/api/python/nn/functional#max.nn.functional.gelu
        # Hint: GELU is the non-linear activation used in GPT-2's MLP
        hidden_states = None

        # 6: Apply the second linear transformation and return
        # TODO: Apply self.c_proj to hidden_states and return the result
        # Hint: This projects back to the embedding dimension
        return None
</code></pre>
<h3 id="validation-1"><a class="header" href="#validation-1">Validation</a></h3>
<p>Run <code>pixi run s02</code> to verify your implementation.</p>
<details>
<summary>Show solution</summary>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Solution for Step 02: Feed-forward Network (MLP)

This module implements the feed-forward network (MLP) used in each
transformer block with GELU activation.
"""

import max.functional as F
from max.nn import Linear, Module
from max.tensor import Tensor
from step_01 import GPT2Config


class GPT2MLP(Module):
    """Feed-forward network matching HuggingFace GPT-2 structure.

    Args:
        intermediate_size: Size of the intermediate layer.
        config: GPT-2 configuration.
    """

    def __init__(self, intermediate_size: int, config: GPT2Config) -&gt; None:
        super().__init__()
        embed_dim = config.n_embd
        self.c_fc = Linear(embed_dim, intermediate_size, bias=True)
        self.c_proj = Linear(intermediate_size, embed_dim, bias=True)

    def forward(self, hidden_states: Tensor) -&gt; Tensor:
        """Apply feed-forward network.

        Args:
            hidden_states: Input hidden states.

        Returns:
            MLP output.
        """
        hidden_states = self.c_fc(hidden_states)
        hidden_states = F.gelu(hidden_states, approximate="tanh")
        hidden_states = self.c_proj(hidden_states)
        return hidden_states
</code></pre>
</details>
<p><strong>Next</strong>: In <a href="./step_03.html">Step 03</a>, you’ll implement causal masking to prevent
tokens from attending to future positions in autoregressive generation.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="causal-masking"><a class="header" href="#causal-masking">Causal masking</a></h1>
<div class="note">
<p>Learn to create attention masks to prevent the model from <em>seeing</em> future tokens
during
<a href="https://docs.modular.com/glossary/ai/autoregression">autoregressive</a>
generation.</p>
</div>
<p>In this step you’ll implement the <code>causal_mask()</code> function that’s required for
self-attention (the next step). This creates a
<a href="https://docs.modular.com/glossary/ai/attention-mask/">mask matrix</a> that
prevents the model from <em>seeing</em> future tokens when predicting the next token.
The mask sets attention scores to negative infinity (<code>-inf</code>) for future
positions. After softmax, these <code>-inf</code> values become zero probability, blocking
information flow from later tokens.</p>
<figure>
<img src="./images/causal-masking-light.png"
  alt="Causal mask matrix with lower triangular pattern"
  class="light-mode-img" width="530" height="475">
<img src="./images/causal-masking-dark.png"
  alt="Causal mask matrix with lower triangular pattern"
  class="dark-mode-img" width="530" height="475">
</figure>
<p>GPT-2 generates text one token at a time, left-to-right. During training, causal
masking prevents the model from “cheating” by looking ahead at tokens it should
be predicting. Without this mask, the model would have access to information it
won’t have during actual text generation.</p>
<h2 id="understanding-the-mask-pattern"><a class="header" href="#understanding-the-mask-pattern">Understanding the mask pattern</a></h2>
<p>The mask creates a lower triangular pattern where each token can only attend to
itself and previous tokens:</p>
<ul>
<li>Position 0 attends to: position 0 only</li>
<li>Position 1 attends to: positions 0-1</li>
<li>Position 2 attends to: positions 0-2</li>
<li>And so on…</li>
</ul>
<p>The mask shape is <code>(sequence_length, sequence_length + num_tokens)</code>. This shape
is designed for <a href="https://docs.modular.com/glossary/ai/kv-cache/">KV cache</a>
compatibility during generation. The KV cache stores key and value tensors from
previously generated tokens, so you only need to compute attention for new
tokens while attending to both new tokens (sequence_length) and cached tokens
(num_tokens). This significantly speeds up generation by avoiding recomputation.</p>
<div class="note">
<div class="title">MAX operations</div>
<p>You’ll use the following MAX operations to complete this task:</p>
<p><strong>Functional decorator</strong>:</p>
<ul>
<li><a href="https://docs.modular.com/max/api/python/nn/functional/#max.nn.functional.functional"><code>@F.functional</code></a>:
Converts functions to graph operations for MAX compilation</li>
</ul>
<p><strong>Tensor operations</strong>:</p>
<ul>
<li><a href="https://docs.modular.com/max/api/python/tensor#max.tensor.Tensor.constant"><code>Tensor.constant()</code></a>:
Creates a scalar constant tensor</li>
<li><a href="https://docs.modular.com/max/api/python/nn/functional#max.nn.functional.broadcast_to"><code>F.broadcast_to()</code></a>:
Expands tensor dimensions to target shape</li>
<li><a href="https://docs.modular.com/max/api/python/nn/functional#max.nn.functional.band_part"><code>F.band_part()</code></a>:
Extracts band matrix (keeps diagonal band, zeros out rest)</li>
</ul>
</div>
<h2 id="implementing-the-mask"><a class="header" href="#implementing-the-mask">Implementing the mask</a></h2>
<p>You’ll create the causal mask in several steps:</p>
<ol>
<li>
<p><strong>Import required modules</strong>:</p>
<ul>
<li><a href="https://docs.modular.com/max/api/python/driver"><code>Device</code></a> from
<code>max.driver</code> - specifies hardware device (CPU/GPU)</li>
<li><a href="https://docs.modular.com/max/api/python/dtype"><code>DType</code></a> from <code>max.dtype</code> -
data type specification</li>
<li><a href="https://docs.modular.com/max/api/python/nn/functional"><code>functional</code></a>
as <code>F</code> from <code>max.nn</code> - functional operations library</li>
<li><a href="https://docs.modular.com/max/api/python/tensor"><code>Tensor</code></a>
from <code>max.tensor</code> - tensor operations</li>
<li><a href="https://docs.modular.com/max/api/python/graph/dim/#max.graph.dim.Dim"><code>Dim</code></a>
from <code>graph.dim</code> - dimension handling</li>
</ul>
</li>
<li>
<p><strong>Add @F.functional decorator</strong>: This converts the function to a MAX graph operation.</p>
</li>
<li>
<p><strong>Calculate total sequence length</strong>: Combine <code>sequence_length</code> and
<code>num_tokens</code> using <code>Dim()</code> to determine mask width.</p>
</li>
<li>
<p><strong>Create constant tensor</strong>: Use
<code>Tensor.constant(float("-inf"), dtype=dtype, device=device)</code> to create a
scalar that will be broadcast.</p>
</li>
<li>
<p><strong>Broadcast to target shape</strong>: Use
<code>F.broadcast_to(mask, shape=(sequence_length, n))</code> to expand the scalar to a
2D matrix.</p>
</li>
<li>
<p><strong>Apply band part</strong>: Use
<code>F.band_part(mask, num_lower=None, num_upper=0, exclude=True)</code> to create the
lower triangular pattern. This keeps 0s on and below the diagonal, <code>-inf</code>
above.</p>
</li>
</ol>
<p><strong>Implementation</strong> (<code>step_03.py</code>):</p>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Step 03: Causal Masking

Implement causal attention masking that prevents tokens from attending to future positions.

Tasks:
1. Import functional module (as F) and Tensor from max.nn
2. Add @F.functional decorator to the causal_mask function
3. Create a constant tensor filled with negative infinity
4. Broadcast the mask to the correct shape (sequence_length, n)
5. Apply band_part to create the lower triangular causal structure

Run: pixi run s03
"""

# 1: Import the required modules from MAX
from max.driver import Device
from max.dtype import DType

# TODO: Import necessary funcional module from max.nn with the alias F
# https://docs.modular.com/max/api/python/nn/functional
# TODO: Import Tensor object from max.tensor
# https://docs.modular.com/max/api/python/tensor.Tensor
from max.graph import Dim, DimLike
from max.tensor import Tensor

# 2: Add the @F.functional decorator to make this a MAX functional operation
# TODO: Add the decorator here


def causal_mask(
    sequence_length: DimLike,
    num_tokens: DimLike,
    *,
    dtype: DType,
    device: Device,
) -&gt; Tensor:
    """Create a causal mask for autoregressive attention.

    Args:
        sequence_length: Length of the sequence.
        num_tokens: Number of tokens.
        dtype: Data type for the mask.
        device: Device to create the mask on.

    Returns:
        A causal mask tensor.
    """
    # Calculate total sequence length
    n = Dim(sequence_length) + num_tokens

    # 3: Create a constant tensor filled with negative infinity
    # TODO: Use Tensor.constant() with float("-inf"), dtype, and device parameters
    # https://docs.modular.com/max/api/python/tensor#max.tensor.Tensor.constant
    # Hint: This creates the base mask value that will block attention to future tokens
    mask = None

    # 4: Broadcast the mask to the correct shape
    # TODO: Use F.broadcast_to() to expand mask to shape (sequence_length, n)
    # https://docs.modular.com/max/api/python/nn/functional#max.nn.functional.broadcast_to
    # Hint: This creates a 2D attention mask matrix
    mask = None

    # 5: Apply band_part to create the causal (lower triangular) structure and return the mask
    # TODO: Use F.band_part() with num_lower=None, num_upper=0, exclude=True
    # https://docs.modular.com/max/api/python/nn/functional/#max.nn.functional.band_part
    # Hint: This keeps only the lower triangle, allowing attention to past tokens only
    return None
</code></pre>
<h3 id="validation-2"><a class="header" href="#validation-2">Validation</a></h3>
<p>Run <code>pixi run s03</code> to verify your implementation.</p>
<details>
<summary>Show solution</summary>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Solution for Step 03: Causal Masking

This module implements causal attention masking that prevents tokens from
attending to future positions in autoregressive generation.
"""

import max.functional as F
from max.driver import Device
from max.dtype import DType
from max.graph import Dim, DimLike
from max.tensor import Tensor


@F.functional
def causal_mask(
    sequence_length: DimLike,
    num_tokens: DimLike,
    *,
    dtype: DType,
    device: Device,
) -&gt; Tensor:
    """Create a causal mask for autoregressive attention.

    Args:
        sequence_length: Length of the sequence.
        num_tokens: Number of tokens.
        dtype: Data type for the mask.
        device: Device to create the mask on.

    Returns:
        A causal mask tensor.
    """
    n = Dim(sequence_length) + num_tokens
    mask = Tensor.constant(float("-inf"), dtype=dtype, device=device)
    mask = F.broadcast_to(mask, shape=(sequence_length, n))
    return F.band_part(mask, num_lower=None, num_upper=0, exclude=True)
</code></pre>
</details>
<p><strong>Next</strong>: In <a href="./step_04.html">Step 04</a>, you’ll implement multi-head attention.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="multi-head-attention"><a class="header" href="#multi-head-attention">Multi-head attention</a></h1>
<div class="note">
<p>Learn to use multi-head
<a href="https://docs.modular.com/glossary/ai/attention/">attention</a>, enabling the model
to attend to different representation subspaces.</p>
</div>
<p>In this step, you’ll implement the <code>GPT2MultiHeadAttention</code> class that runs 12
attention operations in parallel. Instead of computing attention once over the
full 768-dimensional space, you split the dimensions into 12 heads of 64
dimensions each. Each head learns to focus on different patterns.</p>
<p>GPT-2 uses 12 heads with 768-dimensional embeddings, giving each head 768 ÷ 12 =
64 dimensions. The Q, K, V tensors are reshaped to split the embedding dimension
across heads, attention is computed for all heads in parallel, then the outputs
are concatenated back together. This happens in a single efficient operation
using tensor reshaping and broadcasting.</p>
<p>Multiple heads let the model learn complementary attention strategies. Different
heads can specialize in different relationships, such as one that might attend
to adjacent tokens, another to syntactic patterns, and another to semantic
similarity. This increases the model’s capacity without dramatically increasing
computation.</p>
<h2 id="understanding-the-architecture"><a class="header" href="#understanding-the-architecture">Understanding the architecture</a></h2>
<p>Multi-head attention splits the embedding dimension, computes attention
independently for each head, then merges the results. This requires careful
tensor reshaping to organize the computation efficiently.</p>
<p><strong>Head splitting</strong>: Transform from <code>[batch, seq_length, 768]</code> to
<code>[batch, 12, seq_length, 64]</code>. First reshape to add the head dimension:
<code>[batch, seq_length, 12, 64]</code>. Then transpose to move heads before sequence:
<code>[batch, 12, seq_length, 64]</code>. Now each of the 12 heads operates independently
on its 64-dimensional subspace.</p>
<p><strong>Parallel attention</strong>: With shape <code>[batch, num_heads, seq_length, head_dim]</code>,
you can compute attention for all heads simultaneously. The matrix
multiplication <code>Q @ K^T</code> operates on the last two dimensions
<code>[seq_length, head_dim] @ [head_dim, seq_length]</code>, broadcasting across the batch
and head dimensions. All 12 heads computed in a single efficient operation.</p>
<p><strong>Head merging</strong>: Reverse the splitting to go from
<code>[batch, 12, seq_length, 64]</code> back to <code>[batch, seq_length, 768]</code>. First
transpose to <code>[batch, seq_length, 12, 64]</code>, then reshape to flatten the head
dimension: <code>[batch, seq_length, 768]</code>. This concatenates all head outputs back
into the original dimension.</p>
<p><strong>Output projection (<code>c_proj</code>)</strong>: After merging heads, apply a learned linear
transformation that maps <code>[batch, seq_length, 768]</code> to
<code>[batch, seq_length, 768]</code>. This lets the model mix information across heads,
combining the different perspectives each head learned.</p>
<p>The layer names <code>c_attn</code> (combined Q/K/V projection) and <code>c_proj</code> (output
projection) match Hugging Face’s GPT-2 implementation. This naming is essential
for loading pretrained weights.</p>
<div class="note">
<div class="title">MAX operations</div>
<p>You’ll use the following MAX operations to complete this task:</p>
<p><strong>Linear layers</strong>:</p>
<ul>
<li><a href="https://docs.modular.com/max/api/python/nn/module_v3#max.nn.Linear"><code>Linear(in_features, out_features, bias=True)</code></a>:
Q/K/V and output projections</li>
</ul>
<p><strong>Tensor operations</strong>:</p>
<ul>
<li><code>tensor.reshape(new_shape)</code>: Splits or merges head dimension</li>
<li><code>tensor.transpose(axis1, axis2)</code>: Rearranges dimensions for parallel attention</li>
<li><a href="https://docs.modular.com/max/api/python/nn/functional#max.nn.functional.split"><code>F.split(tensor, split_sizes, axis)</code></a>:
Divides Q/K/V from combined projection</li>
</ul>
</div>
<h2 id="implementing-multi-head-attention"><a class="header" href="#implementing-multi-head-attention">Implementing multi-head attention</a></h2>
<p>You’ll create the <code>GPT2MultiHeadAttention</code> class with helper methods for
splitting and merging heads.</p>
<p>First, import the required modules. You’ll need <code>math</code> for scaling, <code>functional as F</code> for operations, <code>Tensor</code> for type hints, <code>device</code> and <code>dtype</code> utilities, and
<code>Linear</code> and <code>Module</code> from MAX’s neural network module. You’ll also need the
<code>causal_mask()</code> function created in step 3.</p>
<p>In the <code>__init__</code> method, create the projection layers and store configuration:</p>
<ul>
<li>Combined Q/K/V projection:
<code>Linear(embed_dim, 3 * embed_dim, bias=True)</code> stored as <code>self.c_attn</code></li>
<li>Output projection: <code>Linear(embed_dim, embed_dim, bias=True)</code> stored as <code>self.c_proj</code></li>
<li>Store <code>self.num_heads</code> (12) and <code>self.head_dim</code> (64) from config</li>
<li>Calculate <code>self.split_size</code> for splitting Q, K, V later</li>
</ul>
<p>Implement <code>_split_heads</code> to reshape for parallel attention:</p>
<ul>
<li>Calculate new shape by replacing the last dimension:
<code>tensor.shape[:-1] + [num_heads, attn_head_size]</code></li>
<li>Reshape to add the head dimension: <code>tensor.reshape(new_shape)</code></li>
<li>Transpose to move heads to position 1: <code>tensor.transpose(-3, -2)</code></li>
<li>Returns shape <code>[batch, num_heads, seq_length, head_size]</code></li>
</ul>
<p>Implement <code>_merge_heads</code> to concatenate head outputs:</p>
<ul>
<li>Transpose to move heads back: <code>tensor.transpose(-3, -2)</code></li>
<li>Calculate flattened shape: <code>tensor.shape[:-2] + [num_heads * attn_head_size]</code></li>
<li>Reshape to merge heads: <code>tensor.reshape(new_shape)</code></li>
<li>Returns shape <code>[batch, seq_length, n_embd]</code></li>
</ul>
<p>Implement <code>_attn</code> to compute scaled dot-product attention for all heads:</p>
<ul>
<li>Compute attention scores: <code>query @ key.transpose(-2, -1)</code></li>
<li>Scale by square root of head dimension</li>
<li>Apply causal mask to prevent attending to future positions</li>
<li>Apply softmax to get attention weights</li>
<li>Multiply weights by values: <code>attn_weights @ value</code></li>
</ul>
<p>In the <code>forward</code> method, orchestrate the complete multi-head attention:</p>
<ul>
<li>Project to Q/K/V: <code>qkv = self.c_attn(hidden_states)</code></li>
<li>Split into separate tensors:
<code>F.split(qkv, [self.split_size, self.split_size, self.split_size], axis=-1)</code></li>
<li>Split heads for each:
<code>query = self._split_heads(query, self.num_heads, self.head_dim)</code> (repeat for
key, value)</li>
<li>Compute attention: <code>attn_output = self._attn(query, key, value)</code></li>
<li>Merge heads:
<code>attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)</code></li>
<li>Final projection: <code>return self.c_proj(attn_output)</code></li>
</ul>
<p><strong>Implementation</strong> (<code>step_04.py</code>):</p>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Step 04: Multi-head Attention

Implement multi-head attention that splits Q/K/V into multiple heads,
computes attention in parallel for each head, and merges the results.

Tasks:
1. Import required modules (math, F, Tensor, Linear, Module, etc.)
2. Create c_attn and c_proj linear layers
3. Implement _split_heads: reshape and transpose to add head dimension
4. Implement _merge_heads: transpose and reshape to remove head dimension
5. Implement _attn: compute attention for all heads in parallel
6. Implement forward pass: project -&gt; split -&gt; attend -&gt; merge -&gt; project

Run: pixi run s04
"""

# TODO: Import required modules
# Hint: You'll need math for scaling
# Hint: You'll need functional as F from max.nn
# Hint: You'll need Tensor, Device, DType from max.tensor and max.driver
# Hint: You'll need Dim, DimLike from max.graph
# Hint: You'll also need Linear and Module from max.nn

from max.tensor import Tensor
from step_01 import GPT2Config

# TODO: Copy causal_mask function from solution_02.py
# This is the same function you implemented in Step 02


class GPT2MultiHeadAttention(Module):
    """Multi-head attention for GPT-2."""

    def __init__(self, config: GPT2Config) -&gt; None:
        super().__init__()

        self.embed_dim = config.n_embd
        self.num_heads = config.n_head
        self.head_dim = self.embed_dim // self.num_heads
        self.split_size = self.embed_dim

        # TODO: Create combined Q/K/V projection
        # Hint: Use Linear(self.embed_dim, 3 * self.embed_dim, bias=True)
        self.c_attn = None

        # TODO: Create output projection
        # Hint: Use Linear(self.embed_dim, self.embed_dim, bias=True)
        self.c_proj = None

    def _split_heads(
        self, tensor: Tensor, num_heads: int, attn_head_size: int
    ) -&gt; Tensor:
        """Split the last dimension into (num_heads, head_size).

        Args:
            tensor: Input tensor, shape [batch, seq_length, n_embd]
            num_heads: Number of attention heads
            attn_head_size: Dimension of each head

        Returns:
            Tensor with shape [batch, num_heads, seq_length, head_size]
        """
        # TODO: Add head dimension
        # Hint: new_shape = tensor.shape[:-1] + [num_heads, attn_head_size]
        # Hint: tensor = tensor.reshape(new_shape)
        pass

        # TODO: Move heads dimension to position 1
        # Hint: return tensor.transpose(-3, -2)
        return None

    def _merge_heads(
        self, tensor: Tensor, num_heads: int, attn_head_size: int
    ) -&gt; Tensor:
        """Merge attention heads back to original shape.

        Args:
            tensor: Input tensor, shape [batch, num_heads, seq_length, head_size]
            num_heads: Number of attention heads
            attn_head_size: Dimension of each head

        Returns:
            Tensor with shape [batch, seq_length, n_embd]
        """
        # TODO: Move heads dimension back
        # Hint: tensor = tensor.transpose(-3, -2)
        pass

        # TODO: Flatten head dimensions
        # Hint: new_shape = tensor.shape[:-2] + [num_heads * attn_head_size]
        # Hint: return tensor.reshape(new_shape)
        return None

    def _attn(self, query: Tensor, key: Tensor, value: Tensor) -&gt; Tensor:
        """Compute attention for all heads in parallel.

        Args:
            query: Query tensor, shape [batch, num_heads, seq_length, head_size]
            key: Key tensor, shape [batch, num_heads, seq_length, head_size]
            value: Value tensor, shape [batch, num_heads, seq_length, head_size]

        Returns:
            Attention output, shape [batch, num_heads, seq_length, head_size]
        """
        # TODO: Implement attention computation
        # The same 5-step process: scores, scale, mask, softmax, weighted sum
        # Hint: Compute attention scores: query @ key.transpose(-1, -2)
        # Hint: Scale by sqrt(head_dim): attn_weights / math.sqrt(head_dim)
        # Hint: Apply causal mask using causal_mask function
        # Hint: Apply softmax: F.softmax(attn_weights)
        # Hint: Weighted sum: attn_weights @ value
        return None

    def forward(self, hidden_states: Tensor) -&gt; Tensor:
        """Apply multi-head attention.

        Args:
            hidden_states: Input tensor, shape [batch, seq_length, n_embd]

        Returns:
            Attention output, shape [batch, seq_length, n_embd]
        """
        # TODO: Project to Q, K, V
        # Hint: qkv = self.c_attn(hidden_states)
        # Hint: query, key, value = F.split(qkv, [self.split_size, self.split_size, self.split_size], axis=-1)
        pass

        # TODO: Split into multiple heads
        # Hint: query = self._split_heads(query, self.num_heads, self.head_dim)
        # Hint: key = self._split_heads(key, self.num_heads, self.head_dim)
        # Hint: value = self._split_heads(value, self.num_heads, self.head_dim)
        pass

        # TODO: Apply attention
        # Hint: attn_output = self._attn(query, key, value)
        pass

        # TODO: Merge heads back
        # Hint: attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)
        pass

        # TODO: Output projection
        # Hint: attn_output = self.c_proj(attn_output)
        # Hint: return attn_output
        return None
</code></pre>
<h3 id="validation-3"><a class="header" href="#validation-3">Validation</a></h3>
<p>Run <code>pixi run s04</code> to verify your implementation.</p>
<details>
<summary>Show solution</summary>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""Solution for Step 04: Multi-head Attention

This module implements multi-head attention, which allows the model to jointly
attend to information from different representation subspaces at different positions.
"""

import math
from typing import cast

import max.functional as F
from max.nn import Linear, Module
from max.tensor import Tensor
from step_01 import GPT2Config
from step_03 import causal_mask


class GPT2MultiHeadAttention(Module):
    """Multi-head attention for GPT-2, matching HuggingFace structure."""

    def __init__(self, config: GPT2Config) -&gt; None:
        """Initialize multi-head attention.

        Args:
            config: GPT2Config containing n_embd and n_head
        """
        super().__init__()

        self.embed_dim = config.n_embd
        self.num_heads = config.n_head
        self.head_dim = self.embed_dim // self.num_heads
        self.split_size = self.embed_dim

        # Combined Q/K/V projection
        self.c_attn = Linear(self.embed_dim, 3 * self.embed_dim, bias=True)
        # Output projection
        self.c_proj = Linear(self.embed_dim, self.embed_dim, bias=True)

    def _split_heads(
        self, tensor: Tensor, num_heads: int, attn_head_size: int
    ) -&gt; Tensor:
        """Split the last dimension into (num_heads, head_size).

        Transforms shape from [batch, seq_length, n_embd]
        to [batch, num_heads, seq_length, head_size]

        Args:
            tensor: Input tensor, shape [batch, seq_length, n_embd]
            num_heads: Number of attention heads
            attn_head_size: Dimension of each head

        Returns:
            Tensor with shape [batch, num_heads, seq_length, head_size]
        """
        # Add head dimension: [batch, seq_length, n_embd] -&gt; [batch, seq_length, num_heads, head_size]
        new_shape = list(tensor.shape[:-1]) + [num_heads, attn_head_size]
        tensor = tensor.reshape(new_shape)
        # Move heads dimension: [batch, seq_length, num_heads, head_size] -&gt; [batch, num_heads, seq_length, head_size]
        return tensor.transpose(-3, -2)

    def _merge_heads(
        self, tensor: Tensor, num_heads: int, attn_head_size: int
    ) -&gt; Tensor:
        """Merge attention heads back to original shape.

        Transforms shape from [batch, num_heads, seq_length, head_size]
        to [batch, seq_length, n_embd]

        Args:
            tensor: Input tensor, shape [batch, num_heads, seq_length, head_size]
            num_heads: Number of attention heads
            attn_head_size: Dimension of each head

        Returns:
            Tensor with shape [batch, seq_length, n_embd]
        """
        # Move heads dimension back: [batch, num_heads, seq_length, head_size] -&gt; [batch, seq_length, num_heads, head_size]
        tensor = tensor.transpose(-3, -2)
        # Flatten head dimensions: [batch, seq_length, num_heads, head_size] -&gt; [batch, seq_length, n_embd]
        new_shape = list(tensor.shape[:-2]) + [num_heads * attn_head_size]
        return tensor.reshape(new_shape)

    def _attn(self, query: Tensor, key: Tensor, value: Tensor) -&gt; Tensor:
        """Compute attention for all heads in parallel.

        Args:
            query: Query tensor, shape [batch, num_heads, seq_length, head_size]
            key: Key tensor, shape [batch, num_heads, seq_length, head_size]
            value: Value tensor, shape [batch, num_heads, seq_length, head_size]

        Returns:
            Attention output, shape [batch, num_heads, seq_length, head_size]
        """
        # Compute attention scores
        attn_weights = query @ key.transpose(-1, -2)

        # Scale attention weights
        attn_weights = attn_weights / math.sqrt(int(value.shape[-1]))

        # Apply causal mask
        seq_len = query.shape[-2]
        mask = causal_mask(seq_len, 0, dtype=query.dtype, device=query.device)
        attn_weights = attn_weights + mask

        # Softmax and weighted sum
        attn_weights = F.softmax(attn_weights)
        attn_output = attn_weights @ value

        return attn_output

    def forward(self, hidden_states: Tensor) -&gt; Tensor:
        """Apply multi-head attention.

        Args:
            hidden_states: Input tensor, shape [batch, seq_length, n_embd]

        Returns:
            Attention output, shape [batch, seq_length, n_embd]
        """
        # Project to Q, K, V
        qkv = self.c_attn(hidden_states)
        split_result = F.split(
            qkv, [self.split_size, self.split_size, self.split_size], axis=-1
        )
        query = cast(Tensor, split_result[0])
        key = cast(Tensor, split_result[1])
        value = cast(Tensor, split_result[2])

        # Split into multiple heads
        query = self._split_heads(query, self.num_heads, self.head_dim)
        key = self._split_heads(key, self.num_heads, self.head_dim)
        value = self._split_heads(value, self.num_heads, self.head_dim)

        # Apply attention
        attn_output = self._attn(query, key, value)

        # Merge heads back
        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)

        # Output projection
        attn_output = self.c_proj(attn_output)

        return attn_output
</code></pre>
</details>
<p><strong>Next</strong>: In <a href="./step_05.html">Step 05</a>, you’ll implement layer normalization to
stabilize activations for effective training.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="layer-normalization"><a class="header" href="#layer-normalization">Layer normalization</a></h1>
<div class="note">
<p>Learn to implement layer normalization for stabilizing neural network training.</p>
</div>
<p>In this step, you’ll create the <code>LayerNorm</code> class that normalizes activations
across the feature dimension. For each input, you compute the mean and variance
across all features, normalize by subtracting the mean and dividing by the
standard deviation, then apply learned weight and bias parameters to scale and
shift the result.</p>
<p>Unlike batch normalization, <a href="https://arxiv.org/abs/1607.06450">layer normalization</a>
works independently for each example. This makes it ideal for transformers - no
dependence on batch size, no tracking running statistics during inference, and
consistent behavior between training and generation.</p>
<p>GPT-2 applies layer normalization before the attention and MLP blocks in each of
its 12 transformer layers. This pre-normalization pattern stabilizes training in
deep networks by keeping activations in a consistent range.</p>
<p>While layer normalization is most critical during training to stabilize
gradients and prevent activations from exploding or vanishing, <strong>it’s still
required during inference</strong>. The pretrained GPT-2 model we’re loading was
trained with layer normalization - its learned weights and biases expect
normalized inputs. Skipping layer normalization during inference would cause
activations to be in completely different ranges than what the model learned
during training, leading to poor or nonsensical outputs.</p>
<h2 id="understanding-the-operation"><a class="header" href="#understanding-the-operation">Understanding the operation</a></h2>
<p>Layer normalization normalizes across the feature dimension (the last dimension)
independently for each example. It learns two parameters per feature: weight
(gamma) for scaling and bias (beta) for shifting.</p>
<p>The normalization follows this formula:</p>
<pre><code class="language-math">output = weight * (x - mean) / sqrt(variance + epsilon) + bias
</code></pre>
<p>The mean and variance are computed across all features in each example. After
normalizing to zero mean and unit variance, the learned weight scales the result
and the learned bias shifts it. The epsilon value (typically 1e-5) prevents
division by zero when variance is very small.</p>
<div class="note">
<div class="title">MAX operations</div>
<p>You’ll use the following MAX operations to complete this task:</p>
<p><strong>Modules</strong>:</p>
<ul>
<li><a href="https://docs.modular.com/max/api/python/nn/module_v3/"><code>Module</code></a>: The Module
class used for eager tensors</li>
</ul>
<p><strong>Tensor initialization</strong>:</p>
<ul>
<li><a href="https://docs.modular.com/max/api/python/tensor#max.tensor.Tensor.ones"><code>Tensor.ones()</code></a>:
Creates tensor filled with 1.0 values</li>
<li><a href="https://docs.modular.com/max/api/python/tensor#max.tensor.Tensor.zeros"><code>Tensor.zeros()</code></a>:
Creates tensor filled with 0.0 values</li>
</ul>
<p><strong>Layer normalization</strong>:</p>
<ul>
<li><a href="https://docs.modular.com/max/api/python/nn/functional#max.nn.functional.layer_norm"><code>F.layer_norm()</code></a>:
Applies layer normalization with parameters: <code>input</code>, <code>gamma</code> (weight), <code>beta</code>
(bias), and <code>epsilon</code></li>
</ul>
</div>
<h2 id="implementing-layer-normalization"><a class="header" href="#implementing-layer-normalization">Implementing layer normalization</a></h2>
<p>You’ll create the <code>LayerNorm</code> class that wraps MAX’s layer normalization function
with learnable parameters. The implementation is straightforward - two
parameters and a single function call.</p>
<p>First, import the required modules. You’ll need <code>functional as F</code> for the layer
norm operation and <code>Tensor</code> for creating parameters.</p>
<p>In the <code>__init__</code> method, create two learnable parameters:</p>
<ul>
<li>Weight: <code>Tensor.ones([dim])</code> stored as <code>self.weight</code> - initialized to ones so
the initial transformation is identity</li>
<li>Bias: <code>Tensor.zeros([dim])</code> stored as <code>self.bias</code> - initialized to zeros so
there’s no initial shift</li>
</ul>
<p>Store the epsilon value as <code>self.eps</code> for numerical stability.</p>
<p>In the <code>forward</code> method, apply layer normalization with
<code>F.layer_norm(x, gamma=self.weight, beta=self.bias, epsilon=self.eps)</code>. This
computes the normalization and applies the learned parameters in one operation.</p>
<p><strong>Implementation</strong> (<code>step_05.py</code>):</p>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Step 05: Layer Normalization

Implement layer normalization that normalizes activations for training stability.

Tasks:
1. Import functional module (as F) and Tensor from max.nn
2. Initialize learnable weight (gamma) and bias (beta) parameters
3. Apply layer normalization using F.layer_norm in the forward pass

Run: pixi run s05
"""

# 1: Import the required modules from MAX
# TODO: Import functional module from max.nn with the alias F
# https://docs.modular.com/max/api/python/nn/functional

# TODO: Import Tensor from max.tensor
# https://docs.modular.com/max/api/python/tensor.Tensor

from max.graph import DimLike
from max.nn import Module
from max.tensor import Tensor


class LayerNorm(Module):
    """Layer normalization module.

    Args:
        dim: Dimension to normalize over.
        eps: Epsilon for numerical stability.
    """

    def __init__(self, dim: DimLike, *, eps: float = 1e-5) -&gt; None:
        super().__init__()
        self.eps = eps

        # 2: Initialize learnable weight and bias parameters
        # TODO: Create self.weight as a Tensor of ones with shape [dim]
        # https://docs.modular.com/max/api/python/tensor#max.tensor.Tensor.ones
        # Hint: This is the gamma parameter in layer normalization
        self.weight = None

        # TODO: Create self.bias as a Tensor of zeros with shape [dim]
        # https://docs.modular.com/max/api/python/tensor#max.tensor.Tensor.zeros
        # Hint: This is the beta parameter in layer normalization
        self.bias = None

    def forward(self, x: Tensor) -&gt; Tensor:
        """Apply layer normalization.

        Args:
            x: Input tensor.

        Returns:
            Normalized tensor.
        """
        # 3: Apply layer normalization and return the result
        # TODO: Use F.layer_norm() with x, gamma=self.weight, beta=self.bias, epsilon=self.eps
        # https://docs.modular.com/max/api/python/nn/functional#max.nn.functional.layer_norm
        # Hint: Layer normalization normalizes across the last dimension
        return None
</code></pre>
<h3 id="validation-4"><a class="header" href="#validation-4">Validation</a></h3>
<p>Run <code>pixi run s05</code> to verify your implementation.</p>
<details>
<summary>Show solution</summary>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Solution for Step 05: Layer Normalization

This module implements layer normalization that normalizes activations
across the embedding dimension for training stability.
"""

import max.functional as F
from max.graph import DimLike
from max.nn import Module
from max.tensor import Tensor


class LayerNorm(Module):
    """Layer normalization module.

    Args:
        dim: Dimension to normalize over.
        eps: Epsilon for numerical stability.
    """

    def __init__(self, dim: DimLike, *, eps: float = 1e-5) -&gt; None:
        super().__init__()
        self.eps = eps
        self.weight = Tensor.ones([dim])
        self.bias = Tensor.zeros([dim])

    def forward(self, x: Tensor) -&gt; Tensor:
        """Apply layer normalization.

        Args:
            x: Input tensor.

        Returns:
            Normalized tensor.
        """
        return F.layer_norm(x, gamma=self.weight, beta=self.bias, epsilon=self.eps)
</code></pre>
</details>
<p><strong>Next</strong>: In <a href="./step_06.html">Step 06</a>, you’ll combine multi-head attention, MLP,
layer norm, and residual connections into a complete transformer block.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="transformer-block"><a class="header" href="#transformer-block">Transformer block</a></h1>
<div class="note">
<p>Learn to combine attention, MLP, layer normalization, and residual connections
into a complete transformer block.</p>
</div>
<p>In this step, you’ll build a GPT-2 transformer block in the <code>GPT2Block</code> class.
The transformer block is the definitive feature of GPT-2 and any other transformer
model. It includes a series of self-attention layers (the multi-head attention block),
a simple feed-forward network (the MLP block), and layer normalization—all of which
you’ve already built in the previous steps.</p>
<p>The block processes input through two sequential operations. First, it applies
layer norm, runs multi-head attention, then adds the result back to the input
(residual connection). Second, it applies another layer norm, runs the MLP, and
adds that result back. This pattern is <code>x = x + sublayer(layer_norm(x))</code>, called
pre-normalization.</p>
<p>GPT-2 uses pre-norm because it stabilizes training in deep networks. By
normalizing before each sublayer instead of after, gradients flow more smoothly
through the network’s 12 stacked blocks.</p>
<h2 id="understanding-the-components-1"><a class="header" href="#understanding-the-components-1">Understanding the components</a></h2>
<p>The transformer block consists of four components, applied in this order:</p>
<p><strong>First layer norm (<code>ln_1</code>)</strong>: Normalizes the input before attention. Uses
epsilon=1e-5 for numerical stability.</p>
<p><strong>Multi-head attention (<code>attn</code>)</strong>: The self-attention mechanism from Step 04.
Lets each position attend to all previous positions.</p>
<p><strong>Second layer norm (<code>ln_2</code>)</strong>: Normalizes before the MLP. Same configuration as
the first.</p>
<p><strong>Feed-forward network (<code>mlp</code>)</strong>: The position-wise MLP from Step 02. Expands to
3,072 dimensions internally (4× the embedding size), then projects back to 768.</p>
<p>The block maintains a constant 768-dimensional representation throughout. Input
shape <code>[batch, seq_length, 768]</code> stays the same after each sublayer, which is
essential for stacking 12 blocks together.</p>
<h2 id="understanding-the-flow"><a class="header" href="#understanding-the-flow">Understanding the flow</a></h2>
<p>Each sublayer follows the pre-norm pattern:</p>
<ol>
<li>Save the input as <code>residual</code></li>
<li>Apply layer normalization to the input</li>
<li>Process through the sublayer (attention or MLP)</li>
<li>Add the original <code>residual</code> back to the output</li>
</ol>
<p>This happens twice per block, once for attention and once for the MLP. The
residual connections let gradients flow directly through the network, preventing
vanishing gradients in deep models.</p>
<p>Component names (<code>ln_1</code>, <code>attn</code>, <code>ln_2</code>, <code>mlp</code>) match Hugging Face’s GPT-2
implementation. This matters for loading pretrained weights in later steps.</p>
<h2 id="implementing-the-block"><a class="header" href="#implementing-the-block">Implementing the block</a></h2>
<p>You’ll create the <code>GPT2Block</code> class by composing the components from earlier
steps. The block takes <code>GPT2Config</code> and creates four sublayers, then applies
them in sequence with residual connections.</p>
<p>First, import the required modules. You’ll need <code>Module</code> from MAX, plus the
previously implemented components: <code>GPT2Config</code>, <code>GPT2MLP</code>,
<code>GPT2MultiHeadAttention</code>, and <code>LayerNorm</code>.</p>
<p>In the <code>__init__</code> method, create the four sublayers:</p>
<ul>
<li><code>ln_1</code>: <code>LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)</code></li>
<li><code>attn</code>: <code>GPT2MultiHeadAttention(config)</code></li>
<li><code>ln_2</code>: <code>LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)</code></li>
<li><code>mlp</code>: <code>GPT2MLP(4 * config.n_embd, config)</code></li>
</ul>
<p>The MLP uses <code>4 * config.n_embd</code> (3,072 dimensions) as its inner dimension,
following the standard transformer ratio.</p>
<p>In the <code>forward</code> method, implement the two sublayer blocks:</p>
<p><strong>Attention block</strong>:</p>
<ol>
<li>Save <code>residual = hidden_states</code></li>
<li>Normalize: <code>hidden_states = self.ln_1(hidden_states)</code></li>
<li>Apply attention: <code>attn_output = self.attn(hidden_states)</code></li>
<li>Add back: <code>hidden_states = attn_output + residual</code></li>
</ol>
<p><strong>MLP block</strong>:</p>
<ol>
<li>Save <code>residual = hidden_states</code></li>
<li>Normalize: <code>hidden_states = self.ln_2(hidden_states)</code></li>
<li>Apply MLP: <code>feed_forward_hidden_states = self.mlp(hidden_states)</code></li>
<li>Add back: <code>hidden_states = residual + feed_forward_hidden_states</code></li>
</ol>
<p>Finally, return <code>hidden_states</code>.</p>
<p><strong>Implementation</strong> (<code>step_06.py</code>):</p>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Step 06: Transformer Block

Combine multi-head attention, MLP, layer normalization, and residual
connections into a complete transformer block.

Tasks:
1. Import Module and all previous solution components
2. Create ln_1, attn, ln_2, and mlp layers
3. Implement forward pass with pre-norm residual pattern

Run: pixi run s06
"""

# TODO: Import required modules
# Hint: You'll need Module from max.nn

from max.tensor import Tensor
from step_01 import GPT2Config


class GPT2Block(Module):
    """Complete GPT-2 transformer block."""

    def __init__(self, config: GPT2Config) -&gt; None:
        """Initialize transformer block.

        Args:
            config: GPT2Config containing model hyperparameters
        """
        super().__init__()

        hidden_size = config.n_embd
        inner_dim = (
            config.n_inner
            if hasattr(config, "n_inner") and config.n_inner is not None
            else 4 * hidden_size
        )

        # TODO: Create first layer norm (before attention)
        # Hint: Use LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
        self.ln_1 = None

        # TODO: Create multi-head attention
        # Hint: Use GPT2MultiHeadAttention(config)
        self.attn = None

        # TODO: Create second layer norm (before MLP)
        # Hint: Use LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
        self.ln_2 = None

        # TODO: Create MLP
        # Hint: Use GPT2MLP(inner_dim, config)
        self.mlp = None

    def forward(self, hidden_states: Tensor) -&gt; Tensor:
        """Apply transformer block.

        Args:
            hidden_states: Input tensor, shape [batch, seq_length, n_embd]

        Returns:
            Output tensor, shape [batch, seq_length, n_embd]
        """
        # TODO: Attention block with residual connection
        # Hint: residual = hidden_states
        # Hint: hidden_states = self.ln_1(hidden_states)
        # Hint: attn_output = self.attn(hidden_states)
        # Hint: hidden_states = attn_output + residual
        pass

        # TODO: MLP block with residual connection
        # Hint: residual = hidden_states
        # Hint: hidden_states = self.ln_2(hidden_states)
        # Hint: feed_forward_hidden_states = self.mlp(hidden_states)
        # Hint: hidden_states = residual + feed_forward_hidden_states
        pass

        # TODO: Return the output
        return None
</code></pre>
<h3 id="validation-5"><a class="header" href="#validation-5">Validation</a></h3>
<p>Run <code>pixi run s06</code> to verify your implementation.</p>
<details>
<summary>Show solution</summary>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Solution for Step 06: Transformer Block

This module implements a complete GPT-2 transformer block, combining
multi-head attention, MLP, layer normalization, and residual connections.
"""

from max.nn import Module
from max.tensor import Tensor
from step_01 import GPT2Config
from step_02 import GPT2MLP
from step_04 import GPT2MultiHeadAttention
from step_05 import LayerNorm


class GPT2Block(Module):
    """Complete GPT-2 transformer block matching HuggingFace structure.

    Architecture (pre-norm):
    1. x = x + attention(layer_norm(x))
    2. x = x + mlp(layer_norm(x))
    """

    def __init__(self, config: GPT2Config) -&gt; None:
        """Initialize transformer block.

        Args:
            config: GPT2Config containing model hyperparameters
        """
        super().__init__()

        hidden_size = config.n_embd
        # Inner dimension for MLP (4x hidden size by default)
        inner_dim = (
            config.n_inner
            if hasattr(config, "n_inner") and config.n_inner is not None
            else 4 * hidden_size
        )

        # First layer norm (before attention)
        self.ln_1 = LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
        # Multi-head attention
        self.attn = GPT2MultiHeadAttention(config)
        # Second layer norm (before MLP)
        self.ln_2 = LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
        # Feed-forward MLP
        self.mlp = GPT2MLP(inner_dim, config)

    def forward(self, hidden_states: Tensor) -&gt; Tensor:
        """Apply transformer block.

        Args:
            hidden_states: Input tensor, shape [batch, seq_length, n_embd]

        Returns:
            Output tensor, shape [batch, seq_length, n_embd]
        """
        # Attention block with residual connection
        residual = hidden_states
        hidden_states = self.ln_1(hidden_states)
        attn_output = self.attn(hidden_states)
        hidden_states = attn_output + residual

        # MLP block with residual connection
        residual = hidden_states
        hidden_states = self.ln_2(hidden_states)
        feed_forward_hidden_states = self.mlp(hidden_states)
        hidden_states = residual + feed_forward_hidden_states

        return hidden_states
</code></pre>
</details>
<p><strong>Next</strong>: In <a href="./step_07.html">Step 07</a>, you’ll stack 12 transformer blocks together
to create the main body of the GPT-2 model.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="stacking-transformer-blocks"><a class="header" href="#stacking-transformer-blocks">Stacking transformer blocks</a></h1>
<div class="note">
<p>Learn to stack 12 transformer blocks with embeddings and final normalization to
create the complete GPT-2 model.</p>
</div>
<p>In this step, you’ll create the body of the GPT-2 model (the <code>MaxGPT2Model</code> module)
as a sequence of transformer blocks (<code>GPT2Block</code>) plus <code>LayerNorm</code>. And because
the model body receives raw token IDs during inference, you’ll also have to first
convert the token IDs into token embeddings that are suitable for processing by the
transformer blocks and the rest of the neural network.</p>
<p>The model processes input in four stages: convert token IDs to embeddings, add
position information, pass through 12 transformer blocks sequentially, and
normalize the final output. Each transformer block refines the representation,
building up from surface patterns in early layers to semantic understanding in
later layers.</p>
<p>GPT-2 uses 12 layers because this depth allows the model to learn complex
patterns while remaining trainable. Fewer layers would limit the model’s
capacity. More layers would increase training difficulty without proportional
gains in quality for a 117M parameter model.</p>
<h2 id="understanding-the-components-2"><a class="header" href="#understanding-the-components-2">Understanding the components</a></h2>
<p>The complete model has four main components:</p>
<p><strong>Token embeddings (<code>wte</code>)</strong>: Maps each token ID to a 768-dimensional vector
using a lookup table with 50,257 entries (one per vocabulary token).</p>
<p><strong>Position embeddings (<code>wpe</code>)</strong>: Maps each position (0 to 1,023) to a
768-dimensional vector. These are added to token embeddings so the model knows
token order.</p>
<p><strong>Transformer blocks (<code>h</code>)</strong>: 12 identical blocks stacked using MAX’s
<a href="https://docs.modular.com/max/api/python/nn/module_v3#max.nn.Sequential"><code>Sequential</code></a>
module. Sequential applies blocks in order, passing each block’s output to the
next.</p>
<p><strong>Final layer norm (<code>ln_f</code>)</strong>: Normalizes the output after all blocks. This
stabilizes the representation before the language model head (added in Step 11)
projects to vocabulary logits.</p>
<h2 id="understanding-the-forward-pass"><a class="header" href="#understanding-the-forward-pass">Understanding the forward pass</a></h2>
<p>The forward method processes token IDs through the model:</p>
<p>First, create position indices using
<a href="https://docs.modular.com/max/api/python/tensor#max.tensor.Tensor.arange"><code>Tensor.arange</code></a>.
Generate positions [0, 1, 2, …, seq_length-1] matching the input’s dtype and
device. This ensures compatibility when adding to embeddings.</p>
<p>Next, look up embeddings. Get token embeddings with <code>self.wte(input_ids)</code> and
position embeddings with <code>self.wpe(position_indices)</code>. Add them together
element-wise, as both are shape <code>[batch, seq_length, 768]</code>.</p>
<p>Then, pass through the transformer blocks with <code>self.h(x)</code>. The
<a href="/max/api/python/nn/module_v3#max.nn.Sequential"><code>Sequential</code></a>
module applies all 12 transformer blocks in order, each refining the representation.</p>
<p>Finally, normalize the output with <code>self.ln_f(x)</code> and return the result. The
output shape matches the input: <code>[batch, seq_length, 768]</code>.</p>
<div class="note">
<div class="title">MAX operations</div>
<p>You’ll use the following MAX operations to complete this task:</p>
<p><strong>Module composition</strong>:</p>
<ul>
<li><a href="https://docs.modular.com/max/api/python/nn/module_v3#max.nn.Sequential"><code>Sequential(*modules)</code></a>:
Chains transformer blocks in sequence</li>
</ul>
<p><strong>Embeddings</strong>:</p>
<ul>
<li><a href="https://docs.modular.com/max/api/python/nn/module_v3#max.nn.Embedding"><code>Embedding(num_embeddings, dim)</code></a>:
Token and position embeddings</li>
</ul>
<p><strong>Position generation</strong>:</p>
<ul>
<li><a href="https://docs.modular.com/max/api/python/tensor#max.tensor.Tensor.arange"><code>Tensor.arange(seq_length, dtype, device)</code></a>:
Creates position indices</li>
</ul>
</div>
<h2 id="implementing-the-model"><a class="header" href="#implementing-the-model">Implementing the model</a></h2>
<p>You’ll create the <code>MaxGPT2Model</code> class by composing embedding layers, transformer
blocks, and layer normalization. The class builds on all the components from
previous steps.</p>
<p>First, import the required modules. You’ll need <code>Tensor</code> for position indices,
<code>Embedding</code>, <code>Module</code>, and <code>Sequential</code> from MAX’s neural network module, plus
the previously implemented <code>GPT2Config</code>, <code>LayerNorm</code>, and <code>GPT2Block</code>.</p>
<p>In the <code>__init__</code> method, create the four components:</p>
<ul>
<li>Token embeddings: <code>Embedding(config.vocab_size, dim=config.n_embd)</code> stored as
<code>self.wte</code></li>
<li>Position embeddings: <code>Embedding(config.n_positions, dim=config.n_embd)</code> stored
as <code>self.wpe</code></li>
<li>Transformer blocks:
<code>Sequential(*(GPT2Block(config) for _ in range(config.n_layer)))</code> stored as
<code>self.h</code></li>
<li>Final layer norm: <code>LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)</code>
stored as <code>self.ln_f</code></li>
</ul>
<p>The <code>Sequential</code> module takes a generator expression that creates 12 identical
<code>GPT2Block</code> instances. The <code>*</code> unpacks them as arguments to <code>Sequential</code>.</p>
<p>In the <code>forward</code> method, implement the four-stage processing:</p>
<ol>
<li>Get the sequence length from <code>input_ids.shape</code></li>
<li>Create position indices: <code>Tensor.arange(seq_length, dtype=input_ids.dtype, device=input_ids.device)</code></li>
<li>Look up embeddings and add them: <code>x = self.wte(input_ids) + self.wpe(position_indices)</code></li>
<li>Apply transformer blocks: <code>x = self.h(x)</code></li>
<li>Apply final normalization: <code>x = self.ln_f(x)</code></li>
<li>Return <code>x</code></li>
</ol>
<p>The position indices must match the input’s dtype and device to ensure the
tensors are compatible for addition.</p>
<p><strong>Implementation</strong> (<code>step_07.py</code>):</p>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Step 07: Stacking Transformer Blocks

Stack multiple transformer blocks with embeddings to create
the complete GPT-2 model architecture.

Tasks:
1. Import Tensor, Embedding, Module, Sequential, and previous components
2. Create token and position embeddings
3. Stack n_layer transformer blocks using Sequential
4. Create final layer normalization
5. Implement forward pass: embeddings -&gt; blocks -&gt; layer norm

Run: pixi run s10
"""

# TODO: Import required modules
# Hint: You'll need Tensor from max.tensor
# Hint: You'll need Embedding, Module, Sequential from max.nn

from max.tensor import Tensor
from step_01 import GPT2Config


class MaxGPT2Model(Module):
    """Complete GPT-2 transformer model."""

    def __init__(self, config: GPT2Config) -&gt; None:
        """Initialize GPT-2 model.

        Args:
            config: GPT2Config containing model hyperparameters
        """
        super().__init__()

        # TODO: Create token embeddings
        # Hint: Use Embedding(config.vocab_size, dim=config.n_embd)
        self.wte = None

        # TODO: Create position embeddings
        # Hint: Use Embedding(config.n_positions, dim=config.n_embd)
        self.wpe = None

        # TODO: Stack transformer blocks
        # Hint: Use Sequential(*(GPT2Block(config) for _ in range(config.n_layer)))
        # This creates config.n_layer blocks (12 for GPT-2 base)
        self.h = None

        # TODO: Create final layer normalization
        # Hint: Use LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)
        self.ln_f = None

    def forward(self, input_ids: Tensor) -&gt; Tensor:
        """Forward pass through the transformer.

        Args:
            input_ids: Token IDs, shape [batch, seq_length]

        Returns:
            Hidden states, shape [batch, seq_length, n_embd]
        """
        # TODO: Get batch size and sequence length
        # Hint: batch_size, seq_length = input_ids.shape
        pass

        # TODO: Get token embeddings
        # Hint: tok_embeds = self.wte(input_ids)
        pass

        # TODO: Get position embeddings
        # Hint: Create position indices with Tensor.arange(seq_length, dtype=input_ids.dtype, device=input_ids.device)
        # Hint: pos_embeds = self.wpe(position_indices)
        pass

        # TODO: Combine embeddings
        # Hint: x = tok_embeds + pos_embeds
        pass

        # TODO: Apply transformer blocks
        # Hint: x = self.h(x)
        pass

        # TODO: Apply final layer norm
        # Hint: x = self.ln_f(x)
        pass

        # TODO: Return the output
        return None
</code></pre>
<h3 id="validation-6"><a class="header" href="#validation-6">Validation</a></h3>
<p>Run <code>pixi run s07</code> to verify your implementation.</p>
<details>
<summary>Show solution</summary>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Solution for Step 07: Stacking Transformer Blocks

This module stacks multiple transformer blocks and adds embeddings
to create the complete GPT-2 transformer architecture.
"""

from max.nn import Embedding, Module, Sequential
from max.tensor import Tensor
from step_01 import GPT2Config
from step_05 import LayerNorm
from step_06 import GPT2Block


class MaxGPT2Model(Module):
    """Complete GPT-2 transformer model matching HuggingFace structure.

    Architecture:
    1. Token embeddings + position embeddings
    2. Stack of n_layer transformer blocks
    3. Final layer normalization
    """

    def __init__(self, config: GPT2Config) -&gt; None:
        """Initialize GPT-2 model.

        Args:
            config: GPT2Config containing model hyperparameters
        """
        super().__init__()

        # Token embeddings (vocabulary -&gt; embeddings)
        self.wte = Embedding(config.vocab_size, dim=config.n_embd)
        # Position embeddings (positions -&gt; embeddings)
        self.wpe = Embedding(config.n_positions, dim=config.n_embd)

        # Stack of transformer blocks
        self.h = Sequential(*(GPT2Block(config) for _ in range(config.n_layer)))

        # Final layer normalization
        self.ln_f = LayerNorm(config.n_embd, eps=config.layer_norm_epsilon)

    def forward(self, input_ids: Tensor) -&gt; Tensor:
        """Forward pass through the transformer.

        Args:
            input_ids: Token IDs, shape [batch, seq_length]

        Returns:
            Hidden states, shape [batch, seq_length, n_embd]
        """
        _, seq_length = input_ids.shape

        # Get token embeddings
        tok_embeds = self.wte(input_ids)

        # Get position embeddings
        pos_embeds = self.wpe(
            Tensor.arange(seq_length, dtype=input_ids.dtype, device=input_ids.device)
        )

        # Combine embeddings
        x = tok_embeds + pos_embeds

        # Apply transformer blocks
        x = self.h(x)

        # Final layer norm
        x = self.ln_f(x)

        return x
</code></pre>
</details>
<p><strong>Next</strong>: In <a href="./step_08.html">Step 08</a>, you’ll add the language modeling head that
projects hidden states to vocabulary logits for text generation.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="language-model-head"><a class="header" href="#language-model-head">Language model head</a></h1>
<div class="note">
<p>Learn to add the final linear projection layer that converts hidden states to
vocabulary logits for next-token prediction.</p>
</div>
<p>In this step, you’ll create the <code>MaxGPT2LMHeadModel</code>, which combines the
model body (<code>MaxGPT2Model</code>) with a head <code>Linear</code> layer, thus completing the
GPT-2 model that can predict next tokens. This class wraps the transformer from step
7 and adds a final linear layer that projects 768-dimensional hidden states to
50,257-dimensional vocabulary logits.</p>
<p>The language model head is a single linear layer without bias. For each position
in the sequence, it outputs a score for every possible next token. Higher scores
indicate the model thinks that token is more likely to come next.</p>
<p>At 768 × 50,257 = 38.6M parameters, the LM head is the single largest component
in GPT-2, representing about 33% of the model’s 117M total parameters. This is
larger than all 12 transformer blocks combined.</p>
<h2 id="understanding-the-projection"><a class="header" href="#understanding-the-projection">Understanding the projection</a></h2>
<p>The language model head performs a simple linear projection using MAX’s
<a href="https://docs.modular.com/max/api/python/nn/module_v3#max.nn.Linear"><code>Linear</code></a>
layer. It maps each 768-dimensional hidden state to 50,257 scores, one per
vocabulary token.</p>
<p>The layer uses <code>bias=False</code>, meaning it only has weights and no bias vector.
This saves 50,257 parameters (about 0.4% of model size). The bias provides
little benefit because the layer normalization before the LM head already
centers the activations. Adding a constant bias to all logits wouldn’t change
the relative probabilities after softmax.</p>
<p>The output is called “logits,” which are raw scores before applying softmax.
Logits can be any real number. During text generation (Step 10), you’ll convert
logits to probabilities with softmax. Working with logits directly enables
techniques like temperature scaling and top-k sampling.</p>
<h2 id="understanding-the-complete-model"><a class="header" href="#understanding-the-complete-model">Understanding the complete model</a></h2>
<p>With the LM head added, you now have the complete GPT-2 architecture:</p>
<ol>
<li><strong>Input</strong>: Token IDs <code>[batch, seq_length]</code></li>
<li><strong>Embeddings</strong>: Token + position <code>[batch, seq_length, 768]</code></li>
<li><strong>Transformer blocks</strong>: 12 blocks process the embeddings
<code>[batch, seq_length, 768]</code></li>
<li><strong>Final layer norm</strong>: Normalizes the output <code>[batch, seq_length, 768]</code></li>
<li><strong>LM head</strong>: Projects to vocabulary <code>[batch, seq_length, 50257]</code></li>
<li><strong>Output</strong>: Logits <code>[batch, seq_length, 50257]</code></li>
</ol>
<p>Each position gets independent logits over the vocabulary. To predict the next
token after position i, you look at the logits at position i. The highest
scoring token is the model’s top prediction.</p>
<div class="note">
<div class="title">MAX operations</div>
<p>You’ll use the following MAX operations to complete this task:</p>
<p><strong>Linear layer</strong>:</p>
<ul>
<li><a href="https://docs.modular.com/max/api/python/nn/module_v3#max.nn.Linear"><code>Linear(in_features, out_features, bias=False)</code></a>:
Projects hidden states to vocabulary logits</li>
</ul>
</div>
<h2 id="implementing-the-language-model"><a class="header" href="#implementing-the-language-model">Implementing the language model</a></h2>
<p>You’ll create the <code>MaxGPT2LMHeadModel</code> class that wraps the transformer with a
language modeling head. The implementation is straightforward, with just two
components and a simple forward pass.</p>
<p>First, import the required modules. You’ll need <code>Linear</code> and <code>Module</code> from MAX,
plus the previously implemented <code>GPT2Config</code> and <code>MaxGPT2Model</code>.</p>
<p>In the <code>__init__</code> method, create two components:</p>
<ul>
<li>Transformer: <code>MaxGPT2Model(config)</code> stored as <code>self.transformer</code></li>
<li>LM head: <code>Linear(config.n_embd, config.vocab_size, bias=False)</code> stored as <code>self.lm_head</code></li>
</ul>
<p>Note the <code>bias=False</code> parameter, which creates a linear layer without bias
terms.</p>
<p>In the <code>forward</code> method, implement a simple two-step process:</p>
<ol>
<li>Get hidden states from the transformer: <code>hidden_states = self.transformer(input_ids)</code></li>
<li>Project to vocabulary logits: <code>logits = self.lm_head(hidden_states)</code></li>
<li>Return <code>logits</code></li>
</ol>
<p>That’s it. The model takes token IDs and returns logits. In the next step,
you’ll use these logits to generate text.</p>
<p><strong>Implementation</strong> (<code>step_08.py</code>):</p>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Step 08: Language Model Head

Add the final projection layer that converts hidden states to vocabulary logits.

Tasks:
1. Import Linear, Module, and previous components
2. Create transformer and lm_head layers
3. Implement forward pass: transformer -&gt; lm_head

Run: pixi run s08
"""

# TODO: Import required modules
# Hint: You'll need Linear and Module from max.nn

from max.tensor import Tensor
from step_01 import GPT2Config


class MaxGPT2LMHeadModel(Module):
    """Complete GPT-2 model with language modeling head."""

    def __init__(self, config: GPT2Config) -&gt; None:
        """Initialize GPT-2 with LM head.

        Args:
            config: GPT2Config containing model hyperparameters
        """
        super().__init__()

        self.config = config

        # TODO: Create the transformer
        # Hint: Use MaxGPT2Model(config)
        self.transformer = None

        # TODO: Create language modeling head
        # Hint: Use Linear(config.n_embd, config.vocab_size, bias=False)
        # Projects from hidden dimension to vocabulary size
        self.lm_head = None

    def forward(self, input_ids: Tensor) -&gt; Tensor:
        """Forward pass through transformer and LM head.

        Args:
            input_ids: Token IDs, shape [batch, seq_length]

        Returns:
            Logits over vocabulary, shape [batch, seq_length, vocab_size]
        """
        # TODO: Get hidden states from transformer
        # Hint: hidden_states = self.transformer(input_ids)
        pass

        # TODO: Project to vocabulary logits
        # Hint: logits = self.lm_head(hidden_states)
        pass

        # TODO: Return logits
        return None
</code></pre>
<h3 id="validation-7"><a class="header" href="#validation-7">Validation</a></h3>
<p>Run <code>pixi run s08</code> to verify your implementation.</p>
<details>
<summary>Show solution</summary>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Solution for Step 08: Language Model Head

This module adds the final projection layer that converts hidden states
to vocabulary logits for predicting the next token.
"""

from max.nn import Linear, Module
from max.tensor import Tensor
from step_01 import GPT2Config
from step_07 import MaxGPT2Model


class MaxGPT2LMHeadModel(Module):
    """Complete GPT-2 model with language modeling head.

    This is the full model that can be used for text generation.
    """

    def __init__(self, config: GPT2Config) -&gt; None:
        """Initialize GPT-2 with LM head.

        Args:
            config: GPT2Config containing model hyperparameters
        """
        super().__init__()

        self.config = config
        # The transformer (embeddings + blocks + final norm)
        self.transformer = MaxGPT2Model(config)
        # Language modeling head (hidden states -&gt; vocabulary logits)
        self.lm_head = Linear(config.n_embd, config.vocab_size, bias=False)

    def forward(self, input_ids: Tensor) -&gt; Tensor:
        """Forward pass through transformer and LM head.

        Args:
            input_ids: Token IDs, shape [batch, seq_length]

        Returns:
            Logits over vocabulary, shape [batch, seq_length, vocab_size]
        """
        # Get hidden states from transformer
        hidden_states = self.transformer(input_ids)

        # Project to vocabulary logits
        logits = self.lm_head(hidden_states)

        return logits
</code></pre>
</details>
<p><strong>Next</strong>: In <a href="./step_09.html">Step 09</a>, you’ll implement tokenization functions to
convert between text and token IDs.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="encode-and-decode-tokens"><a class="header" href="#encode-and-decode-tokens">Encode and decode tokens</a></h1>
<div class="note">
<p>Learn to convert between text and token IDs using tokenizers and MAX tensors.</p>
</div>
<p>In this step, you’ll implement utility functions to bridge the gap between text
and the token IDs your model operates on. The <code>encode_text()</code> function converts
an input string into a tensor of token IDs, while <code>decode_tokens()</code> converts
token IDs into a string.</p>
<p>As you saw when building the model body in step 7 (<code>MaxGPT2Model</code>), the model
must receive input as token IDs (not raw text). The token IDs are integers that
represent pieces of text according to a tokenizer vocabulary. GPT-2 uses a Byte
Pair Encoding (BPE) tokenizer, which breaks text into subword units. For
example, “Hello world” becomes <code>[15496, 995]</code> - two tokens representing the
words.</p>
<p>You’ll use the Hugging Face tokenizer to handle the text-to-token conversion,
then wrap it with functions that work with MAX tensors. This separation keeps
tokenization (a preprocessing step) separate from model inference (tensor
operations).</p>
<h2 id="understanding-tokenization"><a class="header" href="#understanding-tokenization">Understanding tokenization</a></h2>
<p>Tokenization converts text to a list of integers. The GPT-2 tokenizer uses a
vocabulary of 50,257 tokens, where common words get single tokens and rare words
split into subwords.</p>
<p>The HuggingFace tokenizer provides an <code>encode</code> method that takes text and
returns a Python list of token IDs. For example:</p>
<pre><code class="language-python">token_ids = tokenizer.encode("Hello world")  # Returns [15496, 995]
</code></pre>
<p>You can specify <code>max_length</code> and <code>truncation=True</code> to limit sequence length. If
the text exceeds <code>max_length</code>, the tokenizer cuts it off. This prevents memory
issues with very long inputs.</p>
<p>After encoding, you need to convert the Python list to a MAX tensor. Use
<code>Tensor.constant</code> to create a tensor with the token IDs, specifying
<code>dtype=DType.int64</code> (GPT-2 expects 64-bit integers) and the target device.</p>
<p>The tensor needs shape <code>[batch, seq_length]</code> for model input. Wrap the token
list in another list to add the batch dimension: <code>[token_ids]</code> becomes
<code>[[15496, 995]]</code> with shape <code>[1, 2]</code>.</p>
<h2 id="understanding-decoding"><a class="header" href="#understanding-decoding">Understanding decoding</a></h2>
<p>Decoding reverses tokenization: convert token IDs back to text. This requires
moving tensors from GPU to CPU, converting to NumPy, then using the tokenizer’s
<code>decode</code> method.</p>
<p>First, transfer the tensor to CPU with <code>.to(CPU())</code>. MAX tensors can live on GPU
or CPU, but Python libraries like NumPy only work with CPU data.</p>
<p>Next, convert to NumPy using <code>np.from_dlpack</code>. DLPack is a standard that enables
zero-copy tensor sharing between frameworks. The MAX tensor and NumPy array
share the same underlying memory.</p>
<p>If the tensor is 2D (batch dimension present), flatten it to 1D with
<code>.flatten()</code>. The tokenizer expects a flat list of token IDs, not a batched
format.</p>
<p>Finally, convert to a Python list with <code>.tolist()</code> and decode with
<code>tokenizer.decode(token_ids, skip_special_tokens=True)</code>. The
<code>skip_special_tokens=True</code> parameter removes padding and end-of-sequence markers
from the output.</p>
<div class="note">
<div class="title">MAX operations</div>
<p>You’ll use the following MAX operations to complete this task:</p>
<p><strong>Tensor creation</strong>:</p>
<ul>
<li><a href="https://docs.modular.com/max/api/python/tensor#max.tensor.Tensor.constant"><code>Tensor.constant(data, dtype, device)</code></a>:
Creates tensor from Python data</li>
</ul>
<p><strong>Device transfer</strong>:</p>
<ul>
<li><a href="https://docs.modular.com/max/api/python/tensor#max.tensor.Tensor.to"><code>tensor.to(CPU())</code></a>:
Moves tensor to CPU for NumPy conversion</li>
</ul>
<p><strong>NumPy interop</strong>:</p>
<ul>
<li><code>np.from_dlpack(tensor)</code>: Converts MAX tensor to NumPy using DLPack protocol</li>
</ul>
</div>
<h2 id="implementing-tokenization"><a class="header" href="#implementing-tokenization">Implementing tokenization</a></h2>
<p>You’ll create two functions: <code>encode_text</code> to convert strings to tensors, and
<code>decode_tokens</code> to convert tensors back to strings.</p>
<p>First, import the required modules. You’ll need <code>numpy as np</code> for array
operations, <code>CPU</code> from MAX’s driver for device specification, <code>DType</code> for
specifying integer types, and <code>Tensor</code> for creating and manipulating tensors.</p>
<p>In <code>encode_text</code>, implement the encoding and conversion:</p>
<ol>
<li>Encode the text to token IDs using the tokenizer:
<code>token_ids = tokenizer.encode(text, max_length=max_length, truncation=True)</code></li>
<li>Convert to a MAX tensor with batch dimension:
<code>Tensor.constant([token_ids], dtype=DType.int64, device=device)</code></li>
</ol>
<p>Note the <code>[token_ids]</code> wrapping to create the batch dimension. This gives shape
<code>[1, seq_length]</code> instead of just <code>[seq_length]</code>.</p>
<p>In <code>decode_tokens</code>, implement the reverse process with explicit type conversions:</p>
<ol>
<li>Transfer to CPU and convert to NumPy with explicit type annotation:
<code>token_ids_np: np.ndarray = np.from_dlpack(token_ids.to(CPU()))</code></li>
<li>Flatten if needed: <code>if token_ids_np.ndim &gt; 1: token_ids_np = token_ids_np.flatten()</code></li>
<li>Convert to Python list with explicit type annotation:
<code>token_ids_list: list = token_ids_np.tolist()</code></li>
<li>Decode to text: <code>return tokenizer.decode(token_ids_list, skip_special_tokens=True)</code></li>
</ol>
<p>Note the use of separate variable names (<code>token_ids_np</code>, <code>token_ids_list</code>)
instead of reusing the same variable. This makes the type conversions explicit
and improves code clarity: <code>Tensor</code> → <code>np.ndarray</code> → <code>list</code> → <code>str</code>. The
flattening step handles both 1D and 2D tensors, making the function work with
single sequences or batches.</p>
<p><strong>Implementation</strong> (<code>step_09.py</code>):</p>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Step 09: Encode and decode tokens

This module provides utility functions to tokenize input text
and decode token IDs back to text using a tokenizer.

Tasks:
1. Tokenize text and convert to tensor
2. Decode token IDs back to text

Run: pixi run s09
"""

# TODO: Import required modules
# Hint: You'll need numpy as np
# Hint: You'll need CPU from max.driver
# Hint: You'll need DType from max.dtype
# Hint: You'll need Tensor from max.tensor

from max.driver import Device
from max.tensor import Tensor
from transformers import GPT2Tokenizer


def encode_text(
    text: str, tokenizer: GPT2Tokenizer, device: Device, max_length: int = 128
) -&gt; Tensor:
    """Tokenize text and convert to tensor.

    Args:
        text: Input text to tokenize
        tokenizer: HuggingFace tokenizer
        device: Device to place tensor on
        max_length: Maximum sequence length

    Returns:
        Tensor of token IDs with shape [1, seq_length]
    """
    # TODO: Encode text to token IDs
    # Hint: token_ids = tokenizer.encode(text, max_length=max_length, truncation=True)
    pass

    # TODO: Convert to MAX tensor
    # Hint: return Tensor.constant([token_ids], dtype=DType.int64, device=device)
    # Note: Wrap tokens in a list to create batch dimension
    return None


def decode_tokens(token_ids: Tensor, tokenizer: GPT2Tokenizer) -&gt; str:
    """Decode token IDs back to text.

    Args:
        token_ids: Tensor of token IDs
        tokenizer: HuggingFace tokenizer

    Returns:
        Decoded text string
    """
    # TODO: Convert MAX tensor to NumPy array explicitly
    # Hint: Create a new variable with type annotation: token_ids_np: np.ndarray
    # Hint: token_ids_np = np.from_dlpack(token_ids.to(CPU()))
    # Note: This makes the type conversion from Tensor to np.ndarray explicit
    pass

    # TODO: Flatten if needed
    # Hint: if token_ids_np.ndim &gt; 1: token_ids_np = token_ids_np.flatten()
    pass

    # TODO: Convert to Python list explicitly
    # Hint: Create a new variable: token_ids_list: list = token_ids_np.tolist()
    # Note: This makes the conversion from np.ndarray to list explicit
    pass

    # TODO: Decode to text
    # Hint: return tokenizer.decode(token_ids_list, skip_special_tokens=True)
    return None
</code></pre>
<h3 id="validation-8"><a class="header" href="#validation-8">Validation</a></h3>
<p>Run <code>pixi run s09</code> to verify your implementation correctly converts text to
tensors and back.</p>
<details>
<summary>Show solution</summary>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Solution for Step 09: Encode and decode tokens

This module provides utility functions to tokenize input text
and decode token IDs back to text using a tokenizer.
"""

import numpy as np
from max.driver import CPU, Device
from max.dtype import DType
from max.tensor import Tensor
from transformers import GPT2Tokenizer


def encode_text(
    text: str, tokenizer: GPT2Tokenizer, device: Device, max_length: int = 128
) -&gt; Tensor:
    """Tokenize text and convert to tensor."""
    token_ids = tokenizer.encode(text, max_length=max_length, truncation=True)
    return Tensor.constant([token_ids], dtype=DType.int64, device=device)


def decode_tokens(token_ids: Tensor, tokenizer: GPT2Tokenizer) -&gt; str:
    """Decode token IDs back to text."""
    token_ids_np: np.ndarray = np.from_dlpack(token_ids.to(CPU()))
    if token_ids_np.ndim &gt; 1:
        token_ids_np = token_ids_np.flatten()
    token_ids_list: list = token_ids_np.tolist()
    return tokenizer.decode(token_ids_list, skip_special_tokens=True)
</code></pre>
</details>
<p><strong>Next</strong>: In <a href="./step_10.html">Step 10</a>, you’ll implement the text generation loop
that uses these functions to produce coherent text autoregressively.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="text-generation"><a class="header" href="#text-generation">Text generation</a></h1>
<div class="note">
<p>Learn to implement autoregressive text generation with sampling and temperature
control.</p>
</div>
<p>In this final step, you’ll implement the generation loop that produces text one
token at a time. The model predicts the next token, appends it to the sequence,
feeds that into the model again, and repeats until reaching the desired length.</p>
<p>Start with a prompt like “Hello world” (tokens <code>[15496, 995]</code>). The model
predicts the next token, giving you <code>[15496, 995, 318]</code> (“Hello world is”). It
predicts again, producing <code>[15496, 995, 318, 257]</code> (“Hello world is a”). This
process continues, with each prediction feeding back as input for the next.</p>
<p>You’ll implement two generation strategies: greedy decoding (always pick the
highest-scoring token) and sampling (randomly choose according to
probabilities). You’ll also add temperature control to adjust how random or
focused the generation is—a higher temperature produces more variety (more
hallucinations).</p>
<h2 id="understanding-the-generation-loop"><a class="header" href="#understanding-the-generation-loop">Understanding the generation loop</a></h2>
<p>The generation loop is simple: run the model, extract the next token prediction,
append it to the sequence, repeat. Each iteration requires a full forward pass
through all 12 transformer blocks.</p>
<p>The model outputs logits with shape <code>[batch, seq_length, vocab_size]</code>. Since you
only care about predicting the next token, extract the last position:
<code>logits[0, -1, :]</code>. This gives you a vector of 50,257 scores, one per vocabulary
token.</p>
<p>These scores are logits (unnormalized), not probabilities. To convert them to
probabilities, apply softmax. Then you can either pick the highest-probability
token (greedy) or sample from the distribution (random).</p>
<h2 id="understanding-temperature-control"><a class="header" href="#understanding-temperature-control">Understanding temperature control</a></h2>
<p>Temperature scaling adjusts how random the generation is using the formula
<code>scaled_logits = logits / temperature</code>.</p>
<p>For GPT-2, setting the temperature to 1.0 uses the original distribution. With
temperature 0.7, you sharpen the distribution, and high-probability tokens
become even more likely, making generation more focused and deterministic. With
temperature 1.2, you flatten the distribution, and lower-probability tokens get
more chances, making generation more diverse and creative. GPT-2 temperature
must be between 0 and 2.0.</p>
<p>Temperature is applied before softmax. Dividing by a value less than 1 makes
large logits even larger (sharpening), while dividing by a value greater than 1
reduces the differences between logits (flattening).</p>
<h2 id="understanding-sampling-vs-greedy"><a class="header" href="#understanding-sampling-vs-greedy">Understanding sampling vs greedy</a></h2>
<p>Greedy decoding always picks the highest-probability token using
<a href="https://docs.modular.com/max/api/python/nn/functional#max.nn.functional.argmax"><code>F.argmax</code></a>.
It’s fast, deterministic, and simple, but often produces repetitive text because
the model keeps choosing the safest option.</p>
<p>Sampling randomly selects tokens according to their probabilities. Convert
logits to probabilities with <code>F.softmax</code>, transfer to CPU, convert to NumPy with
<code>np.from_dlpack</code>, ensure the array is 1D and has float dtype (required by
<code>np.random.choice</code>), then sample with <code>np.random.choice</code>. You use NumPy because
MAX doesn’t have built-in sampling yet.</p>
<p>Most practical generation uses sampling with temperature control. This balances
creativity with coherence, as the model can explore different possibilities
while still favoring high-quality continuations.</p>
<div class="note">
<div class="title">MAX operations</div>
<p>You’ll use the following MAX operations to complete this task:</p>
<p><strong>Probability operations</strong>:</p>
<ul>
<li><a href="https://docs.modular.com/max/api/python/nn/functional#max.nn.functional.softmax"><code>F.softmax(logits)</code></a>:
Converts logits to probabilities</li>
<li><a href="https://docs.modular.com/max/api/python/nn/functional#max.nn.functional.argmax"><code>F.argmax(logits)</code></a>:
Selects highest-probability token (greedy)</li>
</ul>
<p><strong>Sequence building</strong>:</p>
<ul>
<li><a href="https://docs.modular.com/max/api/python/nn/functional#max.nn.functional.concat"><code>F.concat([seq, new_token], axis=1)</code></a>:
Appends token to sequence</li>
<li><a href="https://docs.modular.com/max/api/python/tensor#max.tensor.Tensor.constant"><code>Tensor.constant(value, dtype, device)</code></a>:
Creates scalar tensors</li>
</ul>
<p><strong>NumPy interop</strong>:</p>
<ul>
<li><code>probs.to(CPU())</code>: Transfers tensor to CPU</li>
<li><code>np.from_dlpack(probs)</code>: Converts MAX tensor to NumPy for sampling</li>
</ul>
</div>
<h2 id="implementing-text-generation"><a class="header" href="#implementing-text-generation">Implementing text generation</a></h2>
<p>You’ll create two functions: <code>generate_next_token</code> that predicts a single token,
and <code>generate</code> that loops to produce full sequences.</p>
<p>First, import the required modules. You’ll need <code>numpy</code> for sampling, <code>CPU</code> from
MAX’s driver, <code>DType</code> for type constants, <code>functional as F</code> for operations like
softmax and argmax, and <code>Tensor</code> for creating tensors.</p>
<p>In <code>generate_next_token</code>, implement the prediction logic:</p>
<ol>
<li>Run the model to get logits: <code>logits = model(input_ids)</code></li>
<li>Extract the last position (next token prediction):
<code>next_token_logits = logits[0, -1, :]</code></li>
<li>If using temperature, scale the logits by dividing by the temperature tensor</li>
<li>For sampling: convert to probabilities with <code>F.softmax</code>, transfer to CPU,
convert to NumPy with explicit type annotation (<code>probs_np: np.ndarray</code>),
flatten if needed, convert to float64 with <code>.astype(np.float64)</code> (required by
<code>np.random.choice</code>), sample with <code>np.random.choice</code>, then convert back to a
MAX tensor</li>
<li>For greedy: use <code>F.argmax</code> to select the highest-scoring token</li>
</ol>
<p>The temperature must be a tensor with the same dtype and device as the logits.
Create it with <code>Tensor.constant(temperature, dtype=..., device=...)</code>.</p>
<p>In <code>generate</code>, implement the generation loop:</p>
<ol>
<li>Initialize with the input: <code>generated_tokens = input_ids</code></li>
<li>Loop <code>max_new_tokens</code> times</li>
<li>Generate the next token:
<code>next_token = generate_next_token(model, generated_tokens, ...)</code></li>
<li>Reshape to 2D: <code>next_token_2d = next_token.reshape([1, -1])</code></li>
<li>Concatenate to the sequence:
<code>generated_tokens = F.concat([generated_tokens, next_token_2d], axis=1)</code></li>
<li>Return the complete sequence</li>
</ol>
<p>The reshape is necessary because <code>concat</code> requires matching dimensions, and the
generated token is 0D (scalar).</p>
<p><strong>Implementation</strong> (<code>step_10.py</code>):</p>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Step 10: Text Generation

Implement autoregressive text generation with sampling and temperature control.

Tasks:
1. Import required modules (numpy, F, Tensor, DType, CPU)
2. Implement the generate_text function with temperature scaling
3. Add sampling logic with temperature control
4. Concatenate new tokens to generate sequences

Run: pixi run s10
"""

# TODO: Import required modules
# Hint: You'll need numpy as np
# Hint: You'll need CPU from max.driver
# Hint: You'll need DType from max.dtype
# Hint: You'll need functional as F from max.nn
# Hint: You'll need Tensor from max.tensor

from max.driver import Device
from max.nn import Module
from transformers import GPT2Tokenizer


def generate_text(
    model: Module,
    tokenizer: GPT2Tokenizer,
    device: Device,
    prompt: str,
    max_new_tokens: int = 50,
    temperature: float = 0.8,
    do_sample: bool = True,
) -&gt; str:
    """Generate text using the Max model.

    Args:
        model: Compiled MAX model
        tokenizer: HuggingFace tokenizer
        device: Device to run on
        prompt: Starting text
        max_new_tokens: Number of tokens to generate
        temperature: Sampling temperature (higher = more random)
        do_sample: Whether to sample or use greedy decoding

    Returns:
        Generated text string
    """
    # TODO: Tokenize the prompt text
    # Hint: Use encode_text(prompt, tokenizer, device, max_length=100)
    generated_tokens = None

    print(f"Starting generation from: '{prompt}'")
    print(
        f"Settings: max_new_tokens={max_new_tokens}, temperature={temperature}, do_sample={do_sample}"
    )
    print("-" * 50)

    # TODO: Implement generation loop for max_new_tokens steps
    # Hint: for step in range(max_new_tokens):
    pass

    # TODO: Get model predictions (logits) for current sequence
    # Hint: logits = model(generated_tokens)

    # TODO: Extract logits for next token prediction
    # Hint: next_token_logits = logits[0, -1, :]
    # Note: Shape is [batch, seq_len, vocab_size], we want last position

    # TODO: Apply temperature scaling if sampling
    # Hint: if do_sample and temperature &gt; 0:
    #     Create a temperature tensor with Tensor.constant()
    #     Divide next_token_logits by temperature
    #     Apply softmax: probs = F.softmax(next_token_logits)
    #     Convert to numpy with explicit type annotation: probs_np: np.ndarray = np.from_dlpack(probs.to(CPU()))
    #     Ensure it's 1D: if probs_np.ndim &gt; 1: probs_np = probs_np.flatten()
    #     Convert to float for np.random.choice: probs_np = probs_np.astype(np.float64)
    #     Sample: next_token_id = np.random.choice(len(probs_np), p=probs_np)
    #     Convert back to tensor: next_token_tensor = Tensor.constant(next_token_id, dtype=DType.int64, device=generated_tokens.device)
    # Note: np.random.choice requires p to be a 1D float array

    # TODO: Use greedy decoding if not sampling
    # Hint: else: next_token_tensor = F.argmax(next_token_logits)

    # TODO: Reshape next token to 2D for concatenation
    # Hint: next_token_2d = next_token_tensor.reshape([1, -1])

    # TODO: Concatenate to growing sequence
    # Hint: generated_tokens = F.concat([generated_tokens, next_token_2d], axis=1)

    # TODO: Print progress every 5 steps
    # Hint: if step % 5 == 0 or step == max_new_tokens - 1:
    #     current_text = decode_tokens(generated_tokens, tokenizer)
    #     print(f"Step {step + 1:2d}: {current_text}")

    # TODO: Decode final generated sequence
    # Hint: final_text = decode_tokens(generated_tokens, tokenizer)
    final_text = None

    print("-" * 50)
    print(f"Final generated text: '{final_text}'")
    return final_text
</code></pre>
<h3 id="validation-9"><a class="header" href="#validation-9">Validation</a></h3>
<p>Run <code>pixi run s10</code> to verify your implementation.</p>
<details>
<summary>Show solution</summary>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Solution for Step 10: Text Generation

This module implements autoregressive text generation using the GPT-2 model.
"""

import max.functional as F
import numpy as np
from max.driver import CPU, Device
from max.dtype import DType
from max.nn import Module
from max.tensor import Tensor
from step_09 import decode_tokens, encode_text
from transformers import GPT2Tokenizer


def generate_text(
    model: Module,
    tokenizer: GPT2Tokenizer,
    device: Device,
    prompt: str,
    max_new_tokens: int = 50,
    temperature: float = 0.8,
    do_sample: bool = True,
) -&gt; str:
    """Generate text using the Max model."""
    generated_tokens = encode_text(prompt, tokenizer, device, max_length=100)

    print(f"Starting generation from: '{prompt}'")
    print(
        f"Settings: max_new_tokens={max_new_tokens}, temperature={temperature}, do_sample={do_sample}"
    )
    print("-" * 50)

    for step in range(max_new_tokens):
        logits = model(generated_tokens)
        next_token_logits = logits[0, -1, :]

        if do_sample and temperature &gt; 0:
            # Simple temperature scaling without top-k
            temp_tensor = Tensor.constant(
                temperature,
                dtype=next_token_logits.dtype,
                device=next_token_logits.device,
            )
            next_token_logits = next_token_logits / temp_tensor
            probs = F.softmax(next_token_logits)

            # Convert to numpy for actual sampling
            # Explicitly convert to 1D float array for np.random.choice
            probs_np: np.ndarray = np.from_dlpack(probs.to(CPU()))
            if probs_np.ndim &gt; 1:
                probs_np = probs_np.flatten()
            probs_np = probs_np.astype(np.float64)
            next_token_id = np.random.choice(len(probs_np), p=probs_np)
            next_token_tensor = Tensor.constant(
                next_token_id, dtype=DType.int64, device=generated_tokens.device
            )
        else:
            next_token_tensor = F.argmax(next_token_logits)

        next_token_2d = next_token_tensor.reshape([1, -1])
        generated_tokens = F.concat([generated_tokens, next_token_2d], axis=1)

        if step % 5 == 0 or step == max_new_tokens - 1:
            current_text = decode_tokens(generated_tokens, tokenizer)
            print(f"Step {step + 1:2d}: {current_text}")

    final_text = decode_tokens(generated_tokens, tokenizer)
    print("-" * 50)
    print(f"Final generated text: '{final_text}'")
    return final_text
</code></pre>
</details>
<p><strong>Next</strong>: In <a href="./step_11.html">Step 11</a>, you’ll load pretrained weights and
interact with your complete GPT-2 implementation!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="load-weights-and-run-model"><a class="header" href="#load-weights-and-run-model">Load weights and run model</a></h1>
<div class="note">
<p>Learn to load pretrained weights from HuggingFace and prepare the model for text
generation.</p>
</div>
<p>With all components implemented, you’re ready to load OpenAI’s pretrained GPT-2
weights and run the model. This step brings everything together: loading weights
from HuggingFace, handling weight format differences, initializing the
tokenizer, and compiling the model for efficient inference.</p>
<p>The HuggingFace <code>transformers</code> library provides OpenAI’s pretrained GPT-2
weights. You’ll load these weights into your MAX implementation, making your
model immediately capable of generating coherent text without training.</p>
<p>However, there’s a complication: HuggingFace’s GPT-2 uses Conv1D layers for its
linear transformations, while your MAX implementation uses standard Linear
layers. These store weights in transposed formats, so you’ll need to transpose
specific weight matrices after loading.</p>
<h2 id="understanding-weight-loading"><a class="header" href="#understanding-weight-loading">Understanding weight loading</a></h2>
<p>Weight loading involves three steps: loading the HuggingFace model, transferring
weights to your MAX model, and transposing Conv1D weights.</p>
<p>First, load the pretrained model with <code>GPT2LMHeadModel.from_pretrained("gpt2")</code>.
This downloads the weights (about 500MB) and returns a PyTorch model with the
exact architecture you’ve implemented.</p>
<p>Next, transfer these weights to your MAX model using
<code>max_model.load_state_dict(hf_model.state_dict())</code>. The <code>state_dict</code> is a
dictionary mapping layer names to weight tensors. Since your MAX model has the
exact same architecture and layer names, this transfer works seamlessly.</p>
<p>Finally, transpose the weights for layers that use Conv1D in HuggingFace:
<code>c_attn</code>, <code>c_proj</code>, and <code>c_fc</code>. Conv1D stores weights in shape
<code>[in_features, out_features]</code>, while Linear expects
<code>[out_features, in_features]</code>. Use the <code>.T</code> property to transpose:
<code>child.weight = child.weight.T</code>.</p>
<h2 id="understanding-model-compilation"><a class="header" href="#understanding-model-compilation">Understanding model compilation</a></h2>
<p>Before you can run text generation, compile the model with
<code>.compile(token_type)</code>. Compilation analyzes the model’s computation graph and
generates optimized code for your hardware.</p>
<p>First, you need to specify the <code>token_type</code> input using <code>TensorType</code>. This tells
the MAX compiler what shape and dtype to expect:</p>
<pre><code class="language-python">token_type = TensorType(
    DType.int64,
    ("batch", "seqlen"),
    device=DeviceRef.from_device(device)
)
</code></pre>
<p>The shape uses symbolic dimensions <code>("batch", "seqlen")</code> rather than concrete
numbers like <code>[1, 20]</code>. This allows the compiled model to handle any batch size
and sequence length, not just fixed dimensions.</p>
<p>Compilation takes a few seconds but only happens once. After compilation,
inference is much faster because MAX has optimized the entire computation graph.</p>
<h2 id="understanding-the-tokenizer"><a class="header" href="#understanding-the-tokenizer">Understanding the tokenizer</a></h2>
<p>Back in step 9, you implemented functions to encode and decode tokens, but both
functions require a <code>tokenizer</code> argument. Now you’ll load that tokenizer from
Hugging Face, using <code>GPT2Tokenizer.from_pretrained("gpt2")</code>,
which downloads the same tokenization rules OpenAI used during training.</p>
<p>Set the padding token to match the end-of-sequence token:
<code>tokenizer.pad_token = tokenizer.eos_token</code>. GPT-2 doesn’t have a dedicated
padding token, so we reuse the EOS token for this purpose.</p>
<p>Then pass the <code>tokenizer</code> to the <code>generate_text()</code> function you created
in step 10 (which passes it to <code>tokenize_text()</code> and <code>decode_tokens()</code>
from step 9).</p>
<h2 id="implementing-the-main-function"><a class="header" href="#implementing-the-main-function">Implementing the main function</a></h2>
<p>You’ll implement the <code>main()</code> function that orchestrates the entire pipeline:
loading models, transferring weights, initializing the tokenizer, compiling the
model, and running an interactive prompt loop.</p>
<p>Start by loading the pretrained HuggingFace model:</p>
<pre><code class="language-python">hf_model = GPT2LMHeadModel.from_pretrained("gpt2")
</code></pre>
<p>Initialize your MAX model with the default device and configuration:</p>
<pre><code class="language-python">_, device = defaults()
config = GPT2Config()
max_model = MaxGPT2LMHeadModel(config)
</code></pre>
<p>The <code>defaults()</code> function returns <code>(dtype, device)</code> tuples. You only need the
device, so use <code>_</code> to ignore the dtype.</p>
<p>Load and transpose the weights:</p>
<pre><code class="language-python">max_model.load_state_dict(hf_model.state_dict())
max_model.to(device)
for name, child in max_model.descendents:
    if isinstance(child, Linear):
        if any(layer_name in name for layer_name in ["c_attn", "c_proj", "c_fc"]):
            child.weight = child.weight.T
</code></pre>
<p>The <code>descendents</code> property gives you all nested modules with their full paths.
Check each child’s name for the Conv1D layers and transpose their weights.</p>
<p>Initialize the tokenizer:</p>
<pre><code class="language-python">tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token
</code></pre>
<p>Compile the model:</p>
<pre><code class="language-python">token_type = TensorType(
    DType.int64, ("batch", "seqlen"), device=DeviceRef.from_device(device)
)
compiled_max_model = max_model.compile(token_type)
</code></pre>
<p>Finally, create an interactive prompt loop where users can input text and see
generated results:</p>
<pre><code class="language-python">try:
    while True:
        user_input = input("Enter your prompt: ").strip()

        if user_input.lower() in ['quit', 'exit', 'q']:
            break

        if not user_input:
            continue

        generated_text = generate_text(
            compiled_max_model,
            tokenizer,
            device,
            user_input,
            max_new_tokens=50,
            temperature=0.8,
            do_sample=True
        )
        print(f"\nGenerated text:\n{generated_text}\n")

except KeyboardInterrupt:
    print("\n\nExiting...")
</code></pre>
<p>The loop continues until the user types ‘quit’, ‘exit’, ‘q’, or presses Ctrl+C.</p>
<p><strong>Implementation</strong> (<code>step_11.py</code>):</p>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Step 11: Load Weights and Run Model

Load pretrained GPT-2 weights from HuggingFace and run the complete model.

Tasks:
1. Load HuggingFace GPT-2 model and weights
2. Initialize MAX model and load state dict
3. Transpose weights for Conv1D-&gt;Linear compatibility
4. Compile model with correct input specification
5. Create interactive generation loop

Run: pixi run s11
"""


def run_model() -&gt; None:
    """Load GPT-2 model, compile it, and run interactive text generation."""

    # TODO: Load HuggingFace model
    # Hint: hf_model = GPT2LMHeadModel.from_pretrained("gpt2")
    # Hint: print(f"Loaded HuggingFace model:\n{hf_model}")
    hf_model = None

    # TODO: Initialize MAX model with device
    # Hint: _, device = defaults()
    # Hint: print(f"Using device: {device}")
    # Hint: config = GPT2Config()
    # Hint: max_model = MaxGPT2LMHeadModel(config)
    device = None
    config = None
    max_model = None

    print(
        f"Model has {config.n_layer} layers, {config.n_head} heads, {config.n_embd} embedding dim"
    )

    # TODO: Load state dict and move to device
    # Hint: max_model.load_state_dict(hf_model.state_dict())
    # Hint: max_model.to(device)

    # TODO: Transpose weights for Linear layers
    # Hint: HuggingFace uses Conv1D which stores weights transposed
    # Hint: for name, child in max_model.descendents:
    #     if isinstance(child, Linear):
    #         if any(layer_name in name for layer_name in ["c_attn", "c_proj", "c_fc"]):
    #             print(f"Transposing {name}: {child.weight.shape}")
    #             child.weight = child.weight.T

    # TODO: Initialize tokenizer
    # Hint: tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    # Hint: tokenizer.pad_token = tokenizer.eos_token
    tokenizer = None

    # TODO: Compile model
    # Hint: print("\nCompiling model...")
    # Hint: Create TensorType with shape ("batch", "seqlen") and int64 dtype
    # Hint: token_type = TensorType(DType.int64, ("batch", "seqlen"), device=DeviceRef.from_device(device))
    # Hint: compiled_max_model = max_model.compile(token_type)
    compiled_max_model = None

    # Interactive prompt loop
    print("\n" + "=" * 50)
    print("Model ready! Enter prompts to generate text.")
    print("Press Ctrl+C or type 'quit' to exit.")
    print("=" * 50 + "\n")

    # TODO: Implement interactive generation loop
    # Hint: try:
    #     while True:
    #         user_input = input("Enter your prompt: ").strip()
    #         if user_input.lower() in ['quit', 'exit', 'q']:
    #             break
    #         if not user_input:
    #             continue
    #         generated_text = generate_text(
    #             compiled_max_model, tokenizer, device, user_input,
    #             max_new_tokens=50, temperature=0.8, do_sample=True
    #         )
    #         print(f"\nGenerated text:\n{generated_text}\n")
    # except KeyboardInterrupt:
    #     print("\n\nExiting...")


if __name__ == "__main__":
    run_model()
</code></pre>
<h3 id="validation-10"><a class="header" href="#validation-10">Validation</a></h3>
<p>Run <code>pixi run s11</code> to verify your implementation.</p>
<details>
<summary>Show solution</summary>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Solution for Step 11: Load weights and run model


"""

from max.dtype import DType
from max.graph import DeviceRef
from max.nn import Linear
from max.tensor import TensorType, defaults
from step_01 import GPT2Config
from step_08 import MaxGPT2LMHeadModel
from step_10 import generate_text
from transformers import GPT2LMHeadModel, GPT2Tokenizer


def run_model() -&gt; None:
    # Load HuggingFace model
    hf_model = GPT2LMHeadModel.from_pretrained("gpt2")
    print(f"Loaded HuggingFace model:\n{hf_model}")

    # Initialize Max model
    _, device = defaults()
    print(f"Using device: {device}")
    config = GPT2Config()
    max_model = MaxGPT2LMHeadModel(config)

    print(
        f"Model has {config.n_layer} layers, {config.n_head} heads, {config.n_embd} embedding dim"
    )

    # Load state dict and transpose weights
    max_model.load_state_dict(hf_model.state_dict())
    max_model.to(device)
    for name, child in max_model.descendants:
        if isinstance(child, Linear):
            if any(layer_name in name for layer_name in ["c_attn", "c_proj", "c_fc"]):
                print(f"Transposing {name}: {child.weight.shape}")
                # The upstream model has conv1d layers instead of linear, which have their weights
                # stored transposed compared to linear
                child.weight = child.weight.T

    # Initialize tokenizer
    tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
    tokenizer.pad_token = tokenizer.eos_token  # Set padding token

    # Compile model
    print("\nCompiling model...")
    token_type = TensorType(
        DType.int64, ("batch", "seqlen"), device=DeviceRef.from_device(device)
    )
    compiled_max_model = max_model.compile(token_type)

    # Interactive prompt loop
    print("\n" + "=" * 50)
    print("Model ready! Enter prompts to generate text.")
    print("Press Ctrl+C or type 'quit' to exit.")
    print("=" * 50 + "\n")

    try:
        while True:
            user_input = input("Enter your prompt: ").strip()

            if user_input.lower() in ["quit", "exit", "q"]:
                print("Exiting...")
                break

            if not user_input:
                print("Please enter a non-empty prompt.\n")
                continue

            print()
            generated_text = generate_text(
                compiled_max_model,
                tokenizer,
                device,
                user_input,
                max_new_tokens=50,
                temperature=0.8,
                do_sample=True,
            )
            print(f"\nGenerated text:\n{generated_text}\n")
            print("-" * 50 + "\n")

    except KeyboardInterrupt:
        print("\n\nExiting...")


if __name__ == "__main__":
    run_model()
</code></pre>
</details>
<p><strong>Congratulations!</strong> You’ve completed built a complete GPT-2 implementation from
scratch.</p>
<p>If code verification passed, you can execute your <code>step_11.py</code> code with
<code>pixi run gpt2</code>.</p>
<h2 id="whats-next"><a class="header" href="#whats-next">What’s next?</a></h2>
<p>You now understand the architectural foundation that powers modern language
models. LLaMA, Mistral, and more build on these same components with incremental
refinements. You have everything you need to implement those refinements
yourself.</p>
<p>Consider extending your implementation with:</p>
<ul>
<li><strong>Grouped-query attention (GQA)</strong>: Reduce memory consumption by sharing
key-value pairs across multiple query heads, as used in LLaMA 2.</li>
<li><strong>Rotary position embeddings (RoPE)</strong>: Replace learned position embeddings
with rotation-based encoding, improving length extrapolation in models like
LLaMA and GPT-NeoX.</li>
<li><strong>SwiGLU activation</strong>: Swap GELU for the gated linear unit variant used in
LLaMA and PaLM.</li>
<li><strong>Mixture of experts (MoE)</strong>: Add sparse expert routing to scale model
capacity efficiently, as in Mixtral and GPT-4.</li>
</ul>
<p>Each refinement builds directly on what you’ve implemented. The attention
mechanism you wrote becomes grouped-query attention with a simple modification
to how you reshape key-value tensors. Your position embeddings can be replaced
with RoPE by changing how you encode positional information. The feed-forward
network you built becomes SwiGLU by adding a gating mechanism.</p>
<p>Pick an architecture that interests you and start building. You’ll find the
patterns are familiar because the fundamentals haven’t changed.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="theme/code-highlighting.js"></script>
        <script src="theme/sidebar.js"></script>
        <script src="theme/init-amplitude.js"></script>
        <script src="theme/warning.js"></script>

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>

    </div>
    </body>
</html>