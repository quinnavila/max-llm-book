<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Language model head - Build an LLM from scratch with MAX</title>


        <!-- Custom HTML head -->
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:ital,wght@0,100..700;1,100..700&display=swap"
            rel="stylesheet">
        
        <link rel="stylesheet" href="theme/css/custom.css">
        <link rel="stylesheet" href="theme/css/highlight.css">
        <link rel="stylesheet" id="theme">
        
        <!-- Additional meta tags -->
        <meta property="og:title" content="Build an LLM from scratch with MAX">
        <meta property="og:description" content="Learn by building LLMs from scratch with Modular's MAX Framework">
        <meta property="og:image" content="https://llm.modular.com/assets/social/modular-gpt-tutorial-metadata.png">
        <meta property="og:url" content="https://llm.modular.com/">
        <meta name="twitter:card" content="summary">
        <meta name="twitter:image:alt" content="MAX Docs">
        <meta name="twitter:title" content="Build an LLM from scratch with MAX">
        <meta name="twitter:description" content="Learn by building LLMs from scratch with Modular's MAX Framework">
        <meta name="twitter:image" content="https://llm.modular.com/assets/social/modular-gpt-tutorial-metadata.png">
        <link rel="icon" type="image/png" href="https://llm.modular.com/assets/icons/m-dark.svg">
        <script>
          !function(){var i="cioanalytics", analytics=(window[i]=window[i]||[]);if(!analytics.initialize)if(analytics.invoked)window.console&&console.error&&console.error("Snippet included twice.");else{analytics.invoked=!0;analytics.methods=["trackSubmit","trackClick","trackLink","trackForm","pageview","identify","reset","group","track","ready","alias","debug","page","once","off","on","addSourceMiddleware","addIntegrationMiddleware","setAnonymousId","addDestinationMiddleware"];analytics.factory=function(e){return function(){var t=Array.prototype.slice.call(arguments);t.unshift(e);analytics.push(t);return analytics}};for(var e=0;e<analytics.methods.length;e++){var key=analytics.methods[e];analytics[key]=analytics.factory(key)}analytics.load=function(key,e){var t=document.createElement("script");t.type="text/javascript";t.async=!0;t.setAttribute('data-global-customerio-analytics-key', i);t.src="https://cdp.customer.io/v1/analytics-js/snippet/" + key + "/analytics.min.js";var n=document.getElementsByTagName("script")[0];n.parentNode.insertBefore(t,n);analytics._writeKey=key;analytics._loadOptions=e};analytics.SNIPPET_VERSION="4.15.3";
            analytics.load(
              "c5c8ad95a28930735be9",
              {
                "integrations": {
                    "Customer.io In-App Plugin": {
                        anonymousInApp: true
                    }
                }
              }
            );
            analytics.page();
          }}();
        </script>

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Inter:ital,opsz,wght@0,14..32,100..900;1,14..32,100..900&display=swap" rel="stylesheet">

        <meta name="description" content="This guide walks you through implementing GPT-2 using Modular’s MAX framework.">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/css/custom.css">
        <link rel="stylesheet" href="theme/css/highlight.css">

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "ayu";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden" checked>
        <button id="vertical-sidebar-toggle" class="vertical-sidebar-toggle" aria-label="Toggle sidebar"></button>
        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <button class="collapse-sidebar" aria-label="Collapse sidebar"></button>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Toggle color mode and talk to us buttons -->
        <script>
            document.addEventListener('click', function (event) {
                if (!event.target.matches('.theme-toggle')) return;
                event.preventDefault();
                const prevTheme = theme;
                html.classList.remove(theme);
                const newTheme = prevTheme === 'ayu' ? 'light' : 'ayu'
                html.classList.add(newTheme);
                theme = newTheme
                localStorage.setItem('mdbook-theme', theme);
            }, false);
            document.addEventListener('click', function() {
                if (!event.target.matches('.log-in')) return;
                event.preventDefault();
                window.amplitude.logEvent('LoginClickedFromGPT2Tutorial');
                window.open('https://developer.modular.com', '_blank');
            });
        </script>

        <div class="page-header">
            <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                <i class="fa fa-bars"></i>
            </label>
            <div id="menu-bar" class="menu-bar">
                <div class="left-buttons">
                    <div class="logo-section">
                        <a class="desktop-logo-link" href="https://modular.com"></a>
                        <a class="mobile-logo-link" href="https://builds.modular.com"></a>
                        <div class="slash">/</div>
                        <a class="internal-link" href="">MAX LLM</a>
                    </div>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Dark</button></li>
                        </ul>
                    </div>
                <div class="right-buttons">
                    <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                        <i class="fa fa-search"></i>
                    </button>
                    <button id="theme-toggle" class="icon-button theme-toggle-btn" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="false" aria-expanded="false">
                        <i class="theme-toggle"></i>
                    </button>
                    <a class="menu-btn print" href="print.html" title="Print this book" aria-label="Print this book">
                        <i id="print-button" class="fa fa-print"></i>
                    </a>
                    <a class="menu-btn" href="https://github.com/modular/max-llm-book" title="Git repository" aria-label="Git repository">
                        <i id="git-repository-button" class="fa fa-github"></i>
                    </a>
                </div>
            </div>
        </div>

        <div id="page-wrapper" class="page-wrapper">
            <div class="page">

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <div id="content" class="content">
                    <main>
                        <h1 id="language-model-head"><a class="header" href="#language-model-head">Language model head</a></h1>
<div class="note">
<p>Learn to add the final linear projection layer that converts hidden states to
vocabulary logits for next-token prediction.</p>
</div>
<p>In this step, you’ll create the <code>MaxGPT2LMHeadModel</code>, which combines the
model body (<code>MaxGPT2Model</code>) with a head <code>Linear</code> layer, thus completing the
GPT-2 model that can predict next tokens. This class wraps the transformer from step
7 and adds a final linear layer that projects 768-dimensional hidden states to
50,257-dimensional vocabulary logits.</p>
<p>The language model head is a single linear layer without bias. For each position
in the sequence, it outputs a score for every possible next token. Higher scores
indicate the model thinks that token is more likely to come next.</p>
<p>At 768 × 50,257 = 38.6M parameters, the LM head is the single largest component
in GPT-2, representing about 33% of the model’s 117M total parameters. This is
larger than all 12 transformer blocks combined.</p>
<h2 id="understanding-the-projection"><a class="header" href="#understanding-the-projection">Understanding the projection</a></h2>
<p>The language model head performs a simple linear projection using MAX’s
<a href="https://docs.modular.com/max/api/python/nn/module_v3#max.nn.Linear"><code>Linear</code></a>
layer. It maps each 768-dimensional hidden state to 50,257 scores, one per
vocabulary token.</p>
<p>The layer uses <code>bias=False</code>, meaning it only has weights and no bias vector.
This saves 50,257 parameters (about 0.4% of model size). The bias provides
little benefit because the layer normalization before the LM head already
centers the activations. Adding a constant bias to all logits wouldn’t change
the relative probabilities after softmax.</p>
<p>The output is called “logits,” which are raw scores before applying softmax.
Logits can be any real number. During text generation (Step 10), you’ll convert
logits to probabilities with softmax. Working with logits directly enables
techniques like temperature scaling and top-k sampling.</p>
<h2 id="understanding-the-complete-model"><a class="header" href="#understanding-the-complete-model">Understanding the complete model</a></h2>
<p>With the LM head added, you now have the complete GPT-2 architecture:</p>
<ol>
<li><strong>Input</strong>: Token IDs <code>[batch, seq_length]</code></li>
<li><strong>Embeddings</strong>: Token + position <code>[batch, seq_length, 768]</code></li>
<li><strong>Transformer blocks</strong>: 12 blocks process the embeddings
<code>[batch, seq_length, 768]</code></li>
<li><strong>Final layer norm</strong>: Normalizes the output <code>[batch, seq_length, 768]</code></li>
<li><strong>LM head</strong>: Projects to vocabulary <code>[batch, seq_length, 50257]</code></li>
<li><strong>Output</strong>: Logits <code>[batch, seq_length, 50257]</code></li>
</ol>
<p>Each position gets independent logits over the vocabulary. To predict the next
token after position i, you look at the logits at position i. The highest
scoring token is the model’s top prediction.</p>
<div class="note">
<div class="title">MAX operations</div>
<p>You’ll use the following MAX operations to complete this task:</p>
<p><strong>Linear layer</strong>:</p>
<ul>
<li><a href="https://docs.modular.com/max/api/python/nn/module_v3#max.nn.Linear"><code>Linear(in_features, out_features, bias=False)</code></a>:
Projects hidden states to vocabulary logits</li>
</ul>
</div>
<h2 id="implementing-the-language-model"><a class="header" href="#implementing-the-language-model">Implementing the language model</a></h2>
<p>You’ll create the <code>MaxGPT2LMHeadModel</code> class that wraps the transformer with a
language modeling head. The implementation is straightforward, with just two
components and a simple forward pass.</p>
<p>First, import the required modules. You’ll need <code>Linear</code> and <code>Module</code> from MAX,
plus the previously implemented <code>GPT2Config</code> and <code>MaxGPT2Model</code>.</p>
<p>In the <code>__init__</code> method, create two components:</p>
<ul>
<li>Transformer: <code>MaxGPT2Model(config)</code> stored as <code>self.transformer</code></li>
<li>LM head: <code>Linear(config.n_embd, config.vocab_size, bias=False)</code> stored as <code>self.lm_head</code></li>
</ul>
<p>Note the <code>bias=False</code> parameter, which creates a linear layer without bias
terms.</p>
<p>In the <code>forward</code> method, implement a simple two-step process:</p>
<ol>
<li>Get hidden states from the transformer: <code>hidden_states = self.transformer(input_ids)</code></li>
<li>Project to vocabulary logits: <code>logits = self.lm_head(hidden_states)</code></li>
<li>Return <code>logits</code></li>
</ol>
<p>That’s it. The model takes token IDs and returns logits. In the next step,
you’ll use these logits to generate text.</p>
<p><strong>Implementation</strong> (<code>step_08.py</code>):</p>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Step 08: Language Model Head

Add the final projection layer that converts hidden states to vocabulary logits.

Tasks:
1. Import Linear, Module, and previous components
2. Create transformer and lm_head layers
3. Implement forward pass: transformer -&gt; lm_head

Run: pixi run s08
"""

# TODO: Import required modules
# Hint: You'll need Linear and Module from max.nn

from max.tensor import Tensor
from step_01 import GPT2Config


class MaxGPT2LMHeadModel(Module):
    """Complete GPT-2 model with language modeling head."""

    def __init__(self, config: GPT2Config) -&gt; None:
        """Initialize GPT-2 with LM head.

        Args:
            config: GPT2Config containing model hyperparameters
        """
        super().__init__()

        self.config = config

        # TODO: Create the transformer
        # Hint: Use MaxGPT2Model(config)
        self.transformer = None

        # TODO: Create language modeling head
        # Hint: Use Linear(config.n_embd, config.vocab_size, bias=False)
        # Projects from hidden dimension to vocabulary size
        self.lm_head = None

    def forward(self, input_ids: Tensor) -&gt; Tensor:
        """Forward pass through transformer and LM head.

        Args:
            input_ids: Token IDs, shape [batch, seq_length]

        Returns:
            Logits over vocabulary, shape [batch, seq_length, vocab_size]
        """
        # TODO: Get hidden states from transformer
        # Hint: hidden_states = self.transformer(input_ids)
        pass

        # TODO: Project to vocabulary logits
        # Hint: logits = self.lm_head(hidden_states)
        pass

        # TODO: Return logits
        return None
</code></pre>
<h3 id="validation"><a class="header" href="#validation">Validation</a></h3>
<p>Run <code>pixi run s08</code> to verify your implementation.</p>
<details>
<summary>Show solution</summary>
<pre><code class="language-python"># ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""
Solution for Step 08: Language Model Head

This module adds the final projection layer that converts hidden states
to vocabulary logits for predicting the next token.
"""

from max.nn import Linear, Module
from max.tensor import Tensor
from step_01 import GPT2Config
from step_07 import MaxGPT2Model


class MaxGPT2LMHeadModel(Module):
    """Complete GPT-2 model with language modeling head.

    This is the full model that can be used for text generation.
    """

    def __init__(self, config: GPT2Config) -&gt; None:
        """Initialize GPT-2 with LM head.

        Args:
            config: GPT2Config containing model hyperparameters
        """
        super().__init__()

        self.config = config
        # The transformer (embeddings + blocks + final norm)
        self.transformer = MaxGPT2Model(config)
        # Language modeling head (hidden states -&gt; vocabulary logits)
        self.lm_head = Linear(config.n_embd, config.vocab_size, bias=False)

    def forward(self, input_ids: Tensor) -&gt; Tensor:
        """Forward pass through transformer and LM head.

        Args:
            input_ids: Token IDs, shape [batch, seq_length]

        Returns:
            Logits over vocabulary, shape [batch, seq_length, vocab_size]
        """
        # Get hidden states from transformer
        hidden_states = self.transformer(input_ids)

        # Project to vocabulary logits
        logits = self.lm_head(hidden_states)

        return logits
</code></pre>
</details>
<p><strong>Next</strong>: In <a href="./step_09.html">Step 09</a>, you’ll implement tokenization functions to
convert between text and token IDs.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="step_07.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next prefetch" href="step_09.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="step_07.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next prefetch" href="step_09.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="theme/code-highlighting.js"></script>
        <script src="theme/sidebar.js"></script>
        <script src="theme/init-amplitude.js"></script>
        <script src="theme/warning.js"></script>


    </div>
    </body>
</html>